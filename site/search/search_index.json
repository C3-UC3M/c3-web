{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Informaci\u00f3n general","text":"<p>El Centro de Computaci\u00f3n Cient\u00edfica de la UC3M es un centro de servicios de computaci\u00f3n y almacenamiento para grupos de I+D+i, OPIs y empresas relacionadas con la investigaci\u00f3n y la innovaci\u00f3n.</p> <p>El C3 proporciona capacidad de c\u00e1lculo, almacenamiento y soporte t\u00e9cnico a sus usuarios para promover y participar en la elaboraci\u00f3n de proyectos de investigaci\u00f3n y desarrollo tecnol\u00f3gico, as\u00ed como para contribuir al desarrollo y fortalecimiento de la capacidad competitiva de sus usuarios.</p> <p>Los computadores del C3 incluyen capacidad de c\u00f3mputo y almacenamiento que exceden los recursos disponibles en sistemas habituales, siendo especialmente adecuados para las simulaciones num\u00e9ricas, de biocomputaci\u00f3n, y de inteligencia artificial, dado su volumen recursos, tanto en CPUs, como en CPUS y en capacidad de almacenamiento fiable.</p> <p></p>"},{"location":"about/","title":"About","text":"<p>SCientific Computing Center (C3) of University Carlos III of Madrid is part of the Research Support Center (CAI). The CAI's mission is to provide researchers with a technological support center that develops and offers the methodologies and resources necessary for the execution of their research and development projects.</p> <p>The C3 has been created by University Carlos III of Madrid and has received funding from Programa Estatal para Impulsar la Investigaci\u00f3n Cient\u00edfico-T\u00e9cnica of the Agencia Estatal de Investigaci\u00f3n (AEI) through the project \"Centro Para El An\u00e1lisis Y Modelado De Sistemas Complejos En Ingenier\u00eda Y Biomedicina\" with reference EQC2021-007184-P, and through the PREDCOV project, which was funded by an agreement between Comunidad de Madrid (Consejer\u00eda de Educaci\u00f3n, Universidades, Ciencia y Portavoc\u00eda) and the University Carlos III of Madrid to develop research projects concerning SARS-COV 2 and the COVID-19 disease financed with the REACT-UE resources from the European Regional Development Fund (ERDF).</p>"},{"location":"courses/","title":"HPC Courses","text":"<p>A collection of recommended HPC courses.</p>"},{"location":"courses/#an-introduction-to-high-performance-computing","title":"An introduction to High Performance Computing","text":""},{"location":"courses/#introduction-to-high-performance-computing","title":"Introduction to High Performance Computing","text":""},{"location":"courses/#hpc-parallel-programming-resources","title":"HPC &amp; Parallel programming resources","text":""},{"location":"courses/#introduction-to-parallel-programming-with-mpi-and-openmp","title":"Introduction to parallel programming with MPI and OpenMP","text":""},{"location":"courses/#introduction-to-python","title":"Introduction to Python","text":""},{"location":"courses/#high-performance-computing-with-python","title":"High-Performance Computing with Python","text":""},{"location":"courses/#using-ipyhton-for-parallel-computing","title":"Using IPyhton for Parallel Computing","text":""},{"location":"courses/#high-performance-scientific-computing-in-c","title":"High-performance scientific computing in C++","text":""},{"location":"courses/#prace-training-course-directive-based-gpu-programming-with-openacc","title":"PRACE Training Course: Directive-based GPU programming with OpenACC","text":""},{"location":"courses/#practical-high-performance-computing-system-administration","title":"Practical: High-Performance Computing System Administration","text":""},{"location":"resources/installed_sw/","title":"meter excel de SW instalado (quitar lo que no est\u00e9 instalado)","text":""},{"location":"resources/system/","title":"System Description","text":"<p>The C3 supercomputing cluster has the following technical specifications:</p> <ul> <li> <p>90 compute nodes with the following specifications per node:</p> <ul> <li>Dual processor backplane with two AMD EPYC 7713 CPUs, each with 64 cores (128 cores total).</li> <li>1024GB DDR4-3200 ECC RDIMM RAM.</li> <li>3.5TB NVMe PCIe Gen 4.0 local SSD storage for scratch space.</li> </ul> </li> <li> <p>5 GPU nodes with the following specifications per node:</p> <ul> <li>Dual processor motherboard with two AMD EPYC 7513 CPUs, each with 32 cores (64 cores total).</li> <li>512GB DDR4 3200 ECC RDIMM RAM.</li> <li>7TB NVMe PCIe Gen 4.0 local SSD storage for scratch space.</li> </ul> </li> <li> <p>42 NVIDIA A40 GPUs (336 Tensor Cores, 84 RT Cores and 48 GB GDDR6 ECC memory each)</p> </li> <li> <p>High availability storage system featuring:</p> <ul> <li>Lustre FS, with 540TB of capacity and high redundancy.</li> <li>NFS with DRAM access, and 64TB of capacity.</li> </ul> </li> <li> <p>100Gbps HDR Infiniband network for computation and data storage.</p> </li> </ul>"},{"location":"user_docs/cluster_access/","title":"Docs","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/cluster_access/#cluster-access","title":"Cluster Access","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/cluster_access/#ssh","title":"SSH","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/cluster_access/#rdp","title":"RDP","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/cluster_access/#transfer-files","title":"Transfer files","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/install_custom_sw_spack/","title":"Install your own SW with Spack","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/installed_sw/","title":"Installed Software","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/installed_sw/#modules","title":"Modules","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/installed_sw/#spack","title":"Spack","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/installed_sw/#custom-software","title":"Custom Software","text":"<p>If you need additional software, you can try to install it through Spack by following this guide. Contact us if you need usspecial software</p>"},{"location":"user_docs/jupyter/","title":"JupyterHub","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/matlab/","title":"MATLAB","text":""},{"location":"user_docs/matlab/#parallel-computing-with-matlab-on-the-uc3m-hpc-cluster","title":"Parallel Computing with MATLAB on the UC3M-HPC Cluster","text":"<p>This document provides the steps to configure MATLAB to submit jobs to a cluster, retrieve results, and debug errors.</p>"},{"location":"user_docs/matlab/#initial-configuration-running-matlab-on-the-hpc-cluster","title":"Initial Configuration - Running MATLAB on the HPC Cluster","text":"<p>This setup is intended for job submission when you are logged directly into the cluster, either through a command-line or graphical interface.  This process needs to be done once per cluster.</p> <p>After logging into the cluster, start MATLAB.  On the Home tab, click <code>Parallel &gt; Discover Clusters\u2026</code> to discover the profile.</p> <p> </p> <p> </p> <p>Follow the prompts to create a new cluster profile.  Jobs will run across multiple nodes on the cluster rather than on the host machine.</p> <p></p>"},{"location":"user_docs/matlab/#alternate","title":"Alternate","text":"<p>After logging into the cluster, start MATLAB. Call <code>configCluster</code> to create a new cluster profile.</p> <pre><code>configCluster\n</code></pre> <p>Jobs will run across multiple nodes on the cluster rather than on the host machine.</p>"},{"location":"user_docs/matlab/#initial-configuration-running-matlab-on-the-desktop","title":"Initial Configuration - Running MATLAB on the Desktop","text":"<p>This setup is intended for job submission when MATLAB is installed on your machine and jobs are run remotely on the cluster. You can download MATLAB here. This setup needs to be done once per cluster, per version of MATLAB installed on your machine.</p> <p>Start MATLAB and run <code>userpath</code></p> <pre><code>userpath\n</code></pre> <p></p> <p>Download the Cluster MATLAB support package from here.  The contents of the ZIP file should be extracted into the folder returned by the call to <code>userpath</code>.</p> <p>Create a new cluster profile</p> <pre><code>configCluster\n</code></pre> <p>Submission to the cluster requires SSH credentials. You will be prompted for username and password or identity file (private key). The username and location of the private key will be stored in MATLAB for future sessions.</p> <p>Jobs will now run on the cluster rather than on the local machine.</p> <p>NOTE: To run jobs on the local machine instead of the cluster, use the Process profile.</p> <pre><code>% Get a handle to the local resources\nc = parcluster('Processes');\n</code></pre> <p></p>"},{"location":"user_docs/matlab/#configuring-jobs","title":"Configuring Jobs","text":"<p>Prior to submitting the job, various scheduler flags can be assigned, such as queue, e-mail, walltime, etc.  [Only YYY is required OR None of these are required.]</p> <pre><code>% REQUIRED\n\n&lt;a id=\"M_M_M_M_54b8\"&gt;&lt;/a&gt;\n% Specify an account\nc.AdditionalProperties.AccountName = 'account-name';\n\n% OPTIONAL\n\n% Specify a constraint\nc.AdditionalProperties.Constraint = 'feature-name';\n\n% Request email notification of job status\nc.AdditionalProperties.EmailAddress = 'user-id@uc3m.es';\n\n% Specify number of GPUs (default: 0)\nc.AdditionalProperties.GPUsPerNode = 1;\n\n% Specify a particular GPU card\nc.AdditionalProperties.GPUCard = 'gpu-card';\n\n% Specify memory to use, per core (default: 4GB)\nc.AdditionalProperties.MemPerCPU = '6GB';\n\n% Specify the partition\nc.AdditionalProperties.Partition = 'partition-name';\n\n% Specify cores per node (default: 0)\nc.AdditionalProperties.ProcsPerNode = 4;\n\n% Set node exclusivity (default: false)\nc.AdditionalProperties.RequireExclusiveNode = true;\n\n% Specify a reservation\nc.AdditionalProperties.Reservation = 'reservation-name';\n\n% Specify the wall time (e.g., 1 day, 5 hours, 30 minutes)\nc.AdditionalProperties.WallTime = '1-05:30';\n</code></pre> <p>To persist changes made to <code>AdditionalProperties</code> between MATLAB sessions, save the profile</p> <pre><code>c.saveProfile\n</code></pre> <p>To see the values of the current configuration options, display <code>AdditionalProperties</code>.</p> <pre><code>c.AdditionalProperties\n</code></pre> <p>Unset a value when no longer needed.</p> <pre><code>% Turn off email notifications\nc.AdditionalProperties.EmailAddress = '';\n\n% Don't request an entire node\nc.AdditionalProperties.RequireExclusiveNode = false;\n</code></pre>"},{"location":"user_docs/matlab/#interactive-jobs-running-matlab-on-the-hpc-cluster","title":"Interactive Jobs - Running MATLAB on the HPC Cluster","text":"<p>To run an interactive pool job on the cluster, continue to use <code>parpool</code> as before.</p> <pre><code>% Get a handle to the cluster\nc = parcluster;\n\n% Open a pool of 64 workers on the cluster\npool = c.parpool(64);\n</code></pre> <p>Rather than running a local pool on the host machine, the pool can now run across multiple nodes on the cluster.</p> <pre><code>% Run a parfor over 1000 iterations\nparfor idx = 1:1000\n    a(idx) = rand;\nend\n</code></pre> <p>Delete the pool when it\u2019s no longer needed.</p> <pre><code>% Delete the pool\npool.delete\n</code></pre>"},{"location":"user_docs/matlab/#independent-batch-job-matlab-on-the-hpc-cluster-or-desktop","title":"Independent Batch Job - MATLAB on the HPC Cluster or Desktop","text":"<p>Use the <code>batch</code> command to submit asynchronous jobs to the cluster. The <code>batch</code> command will return a job object which is used to access the output of the submitted job. See the MATLAB documentation for more help on <code>batch</code>.</p> <pre><code>% Get a handle to the cluster\nc = parcluster;\n\n% Submit job to query where MATLAB is running on the cluster\njob = c.batch(@pwd, 1, {}, 'CurrentFolder', '.');\n\n% Query job for state\njob.State\n\n% If job is finished, fetch the results\njob.fetchOutputs{1}\n\n% Delete the job after results are no longer needed\njob.delete\n</code></pre> <p>To retrieve a list of running or completed jobs, call <code>parcluster</code> to return the cluster object. The cluster object stores an array of jobs that are listed as <code>queued</code>, <code>running</code>, <code>finished</code>, or <code>failed</code>. Retrieve and view the list of jobs as shown below.</p> <pre><code>c = parcluster;\njobs = c.Jobs\n\n% Get a handle to the second job in the list\njob2 = c.Jobs(2);\n</code></pre> <p>Once the job has been selected, fetch the results as previously done.</p> <p><code>fetchOutputs</code> is used to retrieve function output arguments; if calling <code>batch</code> with a script, use <code>load</code> instead. Data that has been written to disk on the cluster needs to be retrieved directly from the file system (e.g., via sftp).</p> <pre><code>% Fetch all results from the second job in the list\njob2.fetchOutputs{:}\n\n% Alternate: Load results if job was a script instead of a function\njob2.load\n</code></pre>"},{"location":"user_docs/matlab/#parallel-batch-job-matlab-on-the-hpc-cluster-or-desktop","title":"Parallel Batch Job - MATLAB on the HPC Cluster or Desktop","text":"<p><code>The</code> <code>batch</code> command can also support parallel workflows. Let\u2019s use the following example for a parallel job, which you should save separately as <code>parallel_example.m</code>.</p> <pre><code>function [sim_t, A] = parallel_example(iter)\n\nif nargin==0\n    iter = 8;\nend\n\ndisp('Start sim')\n\nt0 = tic;\nparfor idx = 1:iter\n    A(idx) = idx;\n    pause(2)\n    idx\nend\nsim_t = toc(t0);\n\ndisp('Sim completed')\n\nsave RESULTS A\n\nend\n</code></pre> <p>This time when using the <code>batch</code> command, also specify a Pool argument.</p> <pre><code>% Get a handle to the cluster\nc = parcluster;\n\n% Submit a batch pool job using 4 workers for 16 simulations\njob = c.batch(@parallel_example, 1, {16}, 'CurrentFolder','.', 'Pool', 4);\n\n% View current job status\njob.State\n\n% Fetch the results after a finished state is retrieved\njob.fetchOutputs{1}\nans =\n    8.8872\n</code></pre> <p>The job ran in 8.89 seconds using four workers. Note that these jobs will always request <code>N+1</code> CPU cores, since one worker is required to manage the batch job and pool of workers. For example, a job that needs eight workers will request nine CPU cores.</p> <p>Run the same simulation again but increase the Pool size. This time, to retrieve the results later, keep track of the job ID.</p> <p>NOTE: For some applications, there will be a diminishing return when allocating too many workers, as the overhead may exceed computation time.</p> <pre><code>% Get a handle to the cluster\nc = parcluster;\n\n% Submit a batch pool job using 8 workers for 16 simulations\njob = c.batch(@parallel_example, 1, {16}, 'CurrentFolder','.', 'Pool', 8);\n\n% Get the job ID\nid = job.ID\nid =\n    4\n\n% Clear job from workspace (as though MATLAB exited)\nclear job\n</code></pre> <p>With a handle to the cluster, the <code>findJob</code> method searches for the job with the specified job ID.</p> <pre><code>% Get a handle to the cluster\nc = parcluster;\n\n% Find the old job\njob = c.findJob('ID', 4);\n\n% Retrieve the state of the job\njob.State\nans =\n    finished\n\n% Fetch the results\njob.fetchOutputs{1};\nans =\n    4.7270\n</code></pre> <p>The job now runs in 4.73 seconds using eight workers. Run code with different number of workers to determine the ideal number to use.</p> <p>Alternatively, to retrieve job results via a graphical user interface, use the Job Monitor (<code>Parallel &gt; Monitor Jobs</code>).</p> <p> </p> <p> </p>"},{"location":"user_docs/matlab/#debugging","title":"Debugging","text":"<p>If a serial job produces an error, call the <code>getDebugLog</code> method to view the error log file.</p> <p>When submitting an independent job, specify the task.</p> <pre><code>c.getDebugLog(job.Tasks)\n</code></pre> <p>For Pool jobs, only specify the job object.</p> <pre><code>c.getDebugLog(job)\n</code></pre> <p>When troubleshooting a job, the cluster admin may request the scheduler ID of the job. This can be derived by calling <code>getTaskSchedulerIDs</code> (call <code>schedID(job)</code> before R2019b).</p> <pre><code>job.getTaskSchedulerIDs()\nans =\n    25539\n</code></pre>"},{"location":"user_docs/matlab/#helper-functions","title":"Helper Functions","text":"Function  Description  Notes  clusterFeatures  Lists cluster features/constraints  clusterGpuCards  Lists cluster GPU cards  clusterPartitionNames  Lists cluster partition/queue names  disableArchiving  Modifies file archiving to resolve file mirroring issues  Applicable only to Desktop  fixConnection  Reestablishes cluster connection (e.g., after reconnection of VPN)  Applicable only to Desktop  willRun  Explains why job is queued"},{"location":"user_docs/matlab/#to-learn-more","title":"To Learn More","text":"<p>To learn more about the MATLAB Parallel Computing Toolbox, check out these resources:</p> <ul> <li>Parallel Computing Overview </li> <li>Parallel Computing Documentation </li> <li>Parallel Computing Coding Examples </li> <li>Parallel Computing Tutorials </li> <li>Parallel Computing Videos </li> <li>Parallel Computing Webinars </li> </ul>"},{"location":"user_docs/slurm/","title":"Slurm","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/slurm/#accounting","title":"Accounting","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/slurm/#launch-jobs","title":"Launch Jobs","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/slurm/#srun","title":"srun","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/slurm/#sbatch","title":"sbatch","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/slurm/#salloc","title":"salloc","text":"<p>PLACEHOLDER</p>"}]}