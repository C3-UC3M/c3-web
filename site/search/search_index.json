{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"General Information","text":"<p>The UC3M Scientific Computing Center is a computing and storage service center for R&amp;D&amp;i groups, public research organizations, and companies involved in research and innovation.</p> <p>The C3 provides computing power, storage, and technical support to its users to promote and participate in the development of research and technological development projects, as well as to contribute to the development and strengthening of its users' competitive capacity.</p> <p>The C3's computers include computing and storage capacity that exceeds the resources available in standard systems, making them particularly suitable for numerical simulations, biocomputing, and artificial intelligence, given their volume of resources, both in CPUs and in reliable storage capacity.</p> <p>Before using the C3 services you have to request access following these instructions.</p> <p></p>"},{"location":"about/","title":"About","text":"<p>SCientific Computing Center (C3) of University Carlos III of Madrid is part of the Research Support Center (CAI). The CAI's mission is to provide researchers with a technological support center that develops and offers the methodologies and resources necessary for the execution of their research and development projects.</p> <p>The C3 has been created by University Carlos III of Madrid and has received funding from Programa Estatal para Impulsar la Investigaci\u00f3n Cient\u00edfico-T\u00e9cnica of the Agencia Estatal de Investigaci\u00f3n (AEI) through the project \"Centro Para El An\u00e1lisis Y Modelado De Sistemas Complejos En Ingenier\u00eda Y Biomedicina\" with reference EQC2021-007184-P, and through the PREDCOV project, which was funded by an agreement between Comunidad de Madrid (Consejer\u00eda de Educaci\u00f3n, Universidades, Ciencia y Portavoc\u00eda) and the University Carlos III of Madrid to develop research projects concerning SARS-COV 2 and the COVID-19 disease financed with the REACT-UE resources from the European Regional Development Fund (ERDF).</p>"},{"location":"cai/","title":"Request Access","text":""},{"location":"cai/#how-to-request-access-to-c3","title":"How to request access to C3","text":"<ol> <li> <p>Register as a UC3M CAI user. Important for UC3M users: you must specify the project code for invoicing.</p> </li> <li> <p>Once you are a CAI user you can request CAI services. See this manual for help. In the description of the requested service please specify:</p> <ol> <li>CPU hours: minimum 1,000 hours, maximum 2,000,000 hours</li> <li>GPU hours: minimum 100 hours, maximum 2,000 hours</li> <li>Distributed storage (Lustre): minimum 1TB, maximum 4TB</li> </ol> </li> <li> <p>Lastly, fill out this form for more specific information about resources and users.</p> </li> </ol> <p>If you need more resources, please contact us.</p>"},{"location":"cai/#how-to-extend-requested-resources","title":"How to extend requested resources","text":"<p>If you need to extend the requested resources you must apply for a new service. Follow Steps 2 and 3.</p>"},{"location":"courses/","title":"HPC Courses","text":"<p>A collection of recommended HPC courses.</p>"},{"location":"courses/#an-introduction-to-high-performance-computing","title":"An introduction to High Performance Computing","text":""},{"location":"courses/#introduction-to-high-performance-computing","title":"Introduction to High Performance Computing","text":""},{"location":"courses/#hpc-parallel-programming-resources","title":"HPC &amp; Parallel programming resources","text":""},{"location":"courses/#introduction-to-parallel-programming-with-mpi-and-openmp","title":"Introduction to parallel programming with MPI and OpenMP","text":""},{"location":"courses/#introduction-to-python","title":"Introduction to Python","text":""},{"location":"courses/#high-performance-computing-with-python","title":"High-Performance Computing with Python","text":""},{"location":"courses/#using-ipython-for-parallel-computing","title":"Using IPython for Parallel Computing","text":""},{"location":"courses/#high-performance-scientific-computing-in-c","title":"High-performance scientific computing in C++","text":""},{"location":"courses/#prace-training-course-directive-based-gpu-programming-with-openacc","title":"PRACE Training Course: Directive-based GPU programming with OpenACC","text":""},{"location":"courses/#practical-high-performance-computing-system-administration","title":"Practical: High-Performance Computing System Administration","text":""},{"location":"courses/#matlab-self-paced-online-courses","title":"MATLAB Self-Paced Online Courses","text":""},{"location":"use_cases/","title":"Use Cases","text":"EpiGraph: an agent-based simulator OSTEOCAD: Osteosarcoma Classification and Segmentation in Computed Tomography Scans Tuberculosis: A Persistent Global Crisis"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/11/24/students-visit-c3/","title":"Students visit C3","text":"<p>Visit by students from the Master's Degree in Computer Science and Engineering to C3N-IA and the new Supercomputing Center at UC3M.</p>"},{"location":"blog/2025/11/03/-welcome-to-c3-site-/","title":"\ud83c\udf89 Welcome to C3 site! \ud83c\udf0d","text":"<p>C3 will provide services to researchers and and industry for high-performance computing, and artificial intelligence.</p>"},{"location":"resources/installed_sw/","title":"Installed Software","text":"<p>Here you will find a list of available software.</p> Tool Version Source Tau 2.31.1 Lmod (tau/2.31.1) LINPACK 2.3.0 Spack (hpl@2.3) IOR 3.3.0 Spack (ior@3.3.0) OpenMP gcc-12.2.0 Lmod (gnu12/12.2.0) OpenMP gcc-14.2.0 Spack (gcc@14.2.0) OpenMP aocc-5.0.0 Spack (aocc@5.0.0) OpenMP aocc-5.0.0 Lmod (aocc-5.0.0) OpenMPI 5.0.7 Lmod (openmpi/5.0.7-ofi) OpenMPI 5.0.7 Lmod (openmpi/5.0.7-ucx) OpenMPI 4.1.4 Lmod (openmpi/4.1.4) OpenMPI 5.0.6 Spack (openmpi@5.0.6) MPICH 4.3.0 Lmod (mpich/4.3.0-ofi) MPICH 4.3.0 Lmod (mpich/4.3.0-ucx) MPICH 3.4.3 Lmod (mpich/3.4.3-ofi) MPICH 3.4.3 Lmod (mpich/3.4.3-ucx) MPICH 4.2.2 Spack (mpich@4.2.2/r4faawj) MPICH 4.2.2 Spack (mpich@4.2.2/ulhdpg3) UCX 1.18.0 Lmod (ucx/1.18.0) UCX 1.11.2 Lmod (ucx/1.11.2) UCX 1.18.0 Spack (ucx@1.18.0) GNU compiler C/C++/ Fortran 12.2.0 Lmod (gnu12/12.2.0) GNU compiler C/C++/ Fortran 14.2.0 Spack (gcc@14.2.0) NVIDIA HPC-SDK 25.1 Lmod NVIDIA HPC-X 2.22 Lmod (hpcx) Python 3.12.0 Lmod (python/3.12.0) Python 3.11.3 Lmod (python/3.11.3) Python 3.13.2 Spack (python@3.13.2) Python 3.11.9 Spack (python@3.11.9) Python 3.10.16 Spack (python@3.10.16) Anaconda 3 Lmod (anaconda/3) lib GSL 2.7.1 Lmod (gsl/2.7.1) lib xml2 2.13.5 Spack (libxml2@2.13.5) HDF5 1.10.8 Lmod (hdf5/1.10.8) HDF5 1.14.5 Spack (hdf5@1.14.5) ArrayFire 3.8.1 Spack (arrayfire@3.8.1) Apptainer 1.4.1 Local vim N/A Local tmux N/A Local emacs N/A Local (login nodes only) FFTW 3.3.10 Lmod (fftw/3.3.10) AMD-FFTW 5.0 Spack (amdfftw@5.0) NetCDF 4.9.0 Lmod (netcdf/4.9.0) PETSc 3.18.1 Lmod (petsc/3.18.1) superLU 5.2.1 Lmod (superlu/5.2.1) openBLAS 0.3.21 Lmod (openblas/0.3.21) openBLAS 0.3.29 Spack (openblas@0.3.29) AMDBLIS 5.0. Spack (amdblis@5.0) LAPACK 3.12.1 Spack (netlib-lapack@3.12.1) AOCL-libFLAME 5.0. Spack (amdlibflame@5.0) FlexiBLAS 3.4.2 Spack (flexiblas@3.4.2) Apache Arrow 18.0.0 Spack (arrow@18.0.0) git N/A Local git 2.47.0 Spack (git@2.47.0) autoconf 2.72 Spack(autoconf@2.72) automake 1.16.5 Spack(automake@1.16.5) cmake 3.24.2 Lmod (cmake/3.24.2) cmake 3.31.5 Spack (cmake@3.31.5) libtool 2.4.7 Spack (libtool@2.4.7) flex 2.6.4 Spack (flex@2.6.4) bison 3.8.2 Spack (bison@3.8.2) R 4.2.1 Lmod (R/4.2.1) julia 1.11.2 Spack (julia@1.11.2) lib geos 3.13.0 Spack (geos@3.13.0) lib gdal 3.10.1 Spack (gdal@3.10.1) lib proj 9.4.1 Spack (proj@9.4.1) lib sqlite3 3.46.0 Spack (sqlite@3.46.0) lib udunits2 2.2.28 Spack (udunits@2.2.28) lib curl 8.11.1 Spack (curl@8.11.1) lib openssl 3.4.0 Spack (openssl@3.4.0) lib boost 1.80.0 Lmod (boost/1.80.0) lib boost 1.87.0 Spack (boost@1.87.0) lib glpk 5.0 Spack (glpk@5.0) lib gmp 6.3.0 Spack (gmp@6.3.0) lib mpfr 4.2.1 Spack (mpfr@4.2.1) lib tbb 2022.0.0 Spack (intel-tbb@2022.0.0) ODBC 1.3-23 Spack (r-rodbc@1.3-23) ODBC 2.3.12 Spack (unixodbc@2.3.12) ffmpeg 7.1 Spack (ffmpeg@7.1) MATLAB R2025a Lmod (matlab/R2025a) MATLAB R2025b Lmod (matlab/R2025b)"},{"location":"resources/slurm/","title":"Slurm Partitions","text":""},{"location":"resources/slurm/#general-partitions-cpu-only","title":"General partitions (CPU-only)","text":"<ul> <li>cpu:<ul> <li>Nodes: 90 (srv[01-45,101-145])</li> <li>Time limit: 2 hours</li> </ul> </li> <li>batch:<ul> <li>Nodes: 90 (srv[01-45,101-145])</li> <li>Time limit: 2 days</li> </ul> </li> <li>large:<ul> <li>Nodes: 90 (srv[01-45,101-145])</li> <li>Time limit: 14 days</li> </ul> </li> </ul>"},{"location":"resources/slurm/#gpu-partitions","title":"GPU partitions","text":"<ul> <li>gpu:<ul> <li>Nodes: 5 (srvgpu[01-05])</li> <li>Time limit: 2 hours</li> <li>GPUs: 42</li> </ul> </li> <li>gpu-batch:<ul> <li>Nodes: 5 (srvgpu[01-05])</li> <li>Time limit: 2 days</li> <li>GPUs: 42</li> </ul> </li> <li>gpu-large:<ul> <li>Nodes: 5 (srvgpu[01-05])</li> <li>Time limit: 14 days</li> <li>GPUs: 42</li> </ul> </li> </ul>"},{"location":"resources/storage/","title":"Data Storage","text":"<p>There are several storage volumes available at C3.</p> <ul> <li>HOME: NFS volume mounted on all login and compute nodes at /home</li> <li>Lustre: parallel file system comprised of 16 OSTs distributed across 2 storage nodes. It is mounted on all login and compute nodes at /lustre</li> <li>Scratch: temporary local NVMe scratch storage available on all compute nodes. You can access it at /scratch, /tmp and /var/tmp after launching a Slurm job. Please note that everything stored here will be automatically deleted after your Slurm job ends</li> </ul>"},{"location":"resources/system/","title":"System Description","text":"<p>The C3 supercomputing cluster has the following technical specifications:</p> <ul> <li> <p>90 compute nodes with the following specifications per node:</p> <ul> <li>Dual processor backplane with two AMD EPYC 7713 CPUs, each with 64 cores (128 cores total).</li> <li>1024GB DDR4-3200 ECC RDIMM RAM.</li> <li>3.5TB PCIe Gen4 NVMe SSD for local scratch space.</li> </ul> </li> <li> <p>5 GPU nodes with the following specifications per node:</p> <ul> <li>Dual processor motherboard with two AMD EPYC 7513 CPUs, each with 32 cores (64 cores total).</li> <li>512GB DDR4 3200 ECC RDIMM RAM.</li> <li>7TB PCIe Gen4 NVMe SSD for local scratch space.</li> </ul> </li> <li> <p>42 NVIDIA A40 GPUs (336 Tensor Cores, 84 RT Cores and 48GB GDDR6 ECC memory each)</p> </li> <li> <p>High availability storage system featuring:</p> <ul> <li>Lustre FS, with 546TB of capacity and high redundancy.</li> <li>NFS with RDMA access, and 48TB of capacity.</li> </ul> </li> <li> <p>100Gbps HDR Infiniband network for computation and data storage.</p> </li> <li> <p>All nodes are running Rocky Linux 8.10.</p> </li> </ul>"},{"location":"user_docs/accounting/","title":"Accounting","text":"<p>Whenever you log in you will see your accumulated compute hours for the current month (only for your user). If you need to check more details, you can run the my_account command from any login node.</p> You will see this after logging in: <pre><code>Resource consumption for user cpetre (current month):\n--------------------------------------------------------------------------------\nCluster/Account/User Utilization 2025-10-01T00:00:00 - 2025-10-28T11:59:59 (2379600 secs)\nUsage reported in TRES Hours\n--------------------------------------------------------------------------------\nCluster                        Account     Login      TRES Name     Used \n--------- ------------------------------ --------- -------------- -------- \nuc3mhpc                cuentadepruebas    cpetre            cpu     3142 \nuc3mhpc                cuentadepruebas    cpetre       gres/gpu        0 \n\nFor more information about resource usage please use the my_account command\n</code></pre>"},{"location":"user_docs/accounting/#how-to-use-my_account","title":"How to use my_account","text":"<p>Here you will find examples of how to use the my_account command.</p>"},{"location":"user_docs/accounting/#accounts-and-users","title":"Accounts and Users","text":"<p>Let us say you want to see the resource consumption of an entire account, not just of your user, for the current month: <pre><code>my_account -a &lt;account_name&gt; -m\n</code></pre></p> Example Output <pre><code>[cpetre@srvlogin02 ~]$ my_account -a cuentadepruebas -m\n--------------------------------------------------------------------------------\nCluster/Account/User Utilization 2025-10-01T00:00:00 - 2025-10-28T12:59:59 (2383200 secs)\nUsage reported in TRES Hours\n--------------------------------------------------------------------------------\nCluster                        Account     Login      TRES Name     Used \n--------- ------------------------------ --------- -------------- -------- \nuc3mhpc                cuentadepruebas                      cpu     3142 \nuc3mhpc                cuentadepruebas                 gres/gpu        0 \n</code></pre> <p>If you need the usage of each user in the Slurm account: <pre><code>my_account -a &lt;account_name&gt; -v -m\n</code></pre></p> Example Output <pre><code>[cpetre@srvlogin02 ~]$ my_account -a cuentadepruebas -v -m\n--------------------------------------------------------------------------------\nCluster/Account/User Utilization 2025-10-01T00:00:00 - 2025-10-28T12:59:59 (2383200 secs)\nUsage reported in TRES Hours\n--------------------------------------------------------------------------------\nCluster                        Account     Login      TRES Name     Used \n--------- ------------------------------ --------- -------------- -------- \nuc3mhpc                cuentadepruebas                      cpu     3142 \nuc3mhpc                cuentadepruebas                 gres/gpu        0 \nuc3mhpc                cuentadepruebas    cpetre            cpu     3142 \nuc3mhpc                cuentadepruebas    cpetre       gres/gpu        0 \nuc3mhpc                cuentadepruebas   pruebas            cpu       44 \nuc3mhpc                cuentadepruebas   pruebas       gres/gpu        0 \n</code></pre> <p>If you need the usage of a specific user within a specific account: <pre><code>my_account -a &lt;account_name&gt; -u &lt;user_name&gt; -m\n</code></pre></p> Example Output <pre><code>[cpetre@srvlogin02 ~]$ my_account -a cuentadepruebas -u pruebas -m\n--------------------------------------------------------------------------------\nCluster/Account/User Utilization 2025-10-01T00:00:00 - 2025-10-28T13:59:59 (2386800 secs)\nUsage reported in TRES Hours\n--------------------------------------------------------------------------------\nCluster                        Account     Login      TRES Name        Used \n--------- ------------------------------ --------- -------------- ----------- \nuc3mhpc                cuentadepruebas   pruebas            cpu          44 \nuc3mhpc                cuentadepruebas   pruebas       gres/gpu           0 \n</code></pre> <p>If you need to check a specific user's usage: <pre><code>my_account -u &lt;user_name&gt; -m\n</code></pre></p> Example Output <pre><code>[cpetre@srvlogin02 ~]$ my_account -u pruebas -m\n--------------------------------------------------------------------------------\nCluster/Account/User Utilization 2025-10-01T00:00:00 - 2025-10-28T13:59:59 (2386800 secs)\nUsage reported in TRES Hours\n--------------------------------------------------------------------------------\nCluster                        Account     Login      TRES Name        Used \n--------- ------------------------------ --------- -------------- ----------- \nuc3mhpc                cuentadepruebas   pruebas            cpu          44 \nuc3mhpc                cuentadepruebas   pruebas       gres/gpu           0 \n</code></pre>"},{"location":"user_docs/accounting/#date-related-options","title":"Date related options","text":"<p>To show the usage during the current month use the -m flag: <pre><code>my_account -a &lt;account_name&gt; -m\n</code></pre></p> Example Output <pre><code>[cpetre@srvlogin02 ~]$ my_account -a cuentadepruebas -m\n--------------------------------------------------------------------------------\nCluster/Account/User Utilization 2025-10-01T00:00:00 - 2025-10-28T12:59:59 (2383200 secs)\nUsage reported in TRES Hours\n--------------------------------------------------------------------------------\nCluster                        Account     Login      TRES Name     Used \n--------- ------------------------------ --------- -------------- -------- \nuc3mhpc                cuentadepruebas                      cpu     3142 \nuc3mhpc                cuentadepruebas                 gres/gpu        0 \n</code></pre> <p>To show the all-time usage (since the account was created until now) use the -t flag: <pre><code>my_account -a &lt;account_name&gt; -t\n</code></pre></p> Example Output <pre><code>[cpetre@srvlogin02 ~]$ my_account -a cuentadepruebas -t\n--------------------------------------------------------------------------------\nCluster/Account/User Utilization 1970-01-01T00:00:00 - 2025-10-27T23:59:59 (1761609600 secs)\nUsage reported in TRES Hours\n--------------------------------------------------------------------------------\nCluster                        Account     Login      TRES Name       Used \n--------- ------------------------------ --------- -------------- ---------- \nuc3mhpc                cuentadepruebas                      cpu    2364799 \nuc3mhpc                cuentadepruebas                 gres/gpu        953 \n</code></pre> <p>You can also specify a custom date range with the -s and -e options. Note that the date format must be YYYY-MM-DDTHH:MM:SS: <pre><code>my_account -a &lt;account_name&gt; -s &lt;YYYY-MM-DDTHH:MM:SS&gt; -e &lt;YYYY-MM-DDTHH:MM:SS&gt;\n</code></pre></p> Example Output <pre><code>[cpetre@srvlogin02 ~]$ my_account -a cuentadepruebas -s 2025-09-01T00:00:00 -e 2025-10-31T23:59:59\n--------------------------------------------------------------------------------\nCluster/Account/User Utilization 2025-09-01T00:00:00 - 2025-10-28T12:59:59 (4975200 secs)\nUsage reported in TRES Hours\n--------------------------------------------------------------------------------\nCluster                        Account     Login      TRES Name     Used \n--------- ------------------------------ --------- -------------- -------- \nuc3mhpc                cuentadepruebas                      cpu    19027 \nuc3mhpc                cuentadepruebas                 gres/gpu      109 \n</code></pre>"},{"location":"user_docs/accounting/#multiple-accountsusers","title":"Multiple accounts/users","text":"<p>If you need to display the usage of multiple Slurm accounts and/or users, you can specify a comma-separated list of them: <pre><code>my_account -a &lt;account_name&gt;[,&lt;account_name&gt;]* -t\n</code></pre></p> Example Output <pre><code>[cpetre@srvlogin02 ~]$ my_account -a cuentadepruebas,acct_tests_1 -t\n--------------------------------------------------------------------------------\nCluster/Account/User Utilization 1970-01-01T00:00:00 - 2025-10-27T23:59:59 (1761609600 secs)\nUsage reported in TRES Hours\n--------------------------------------------------------------------------------\nCluster                        Account     Login      TRES Name     Used \n--------- ------------------------------ --------- -------------- -------- \nuc3mhpc                   acct_tests_1                      cpu        1 \nuc3mhpc                   acct_tests_1                 gres/gpu        0 \nuc3mhpc                cuentadepruebas                      cpu  2364799 \nuc3mhpc                cuentadepruebas                 gres/gpu      953 \n</code></pre> <pre><code>my_account -u &lt;user_name&gt;[,&lt;user_name&gt;]* -t\n</code></pre> Example Output <pre><code>[cpetre@srvlogin02 ~]$ my_account -u cpetre,pruebas -t\n--------------------------------------------------------------------------------\nCluster/Account/User Utilization 1970-01-01T00:00:00 - 2025-10-27T23:59:59 (1761609600 secs)\nUsage reported in TRES Hours\n--------------------------------------------------------------------------------\nCluster                        Account     Login      TRES Name     Used \n--------- ------------------------------ --------- -------------- -------- \nuc3mhpc                cuentadepruebas    cpetre            cpu     7242 \nuc3mhpc                cuentadepruebas    cpetre       gres/gpu        2 \nuc3mhpc                cuentadepruebas   pruebas            cpu       44 \nuc3mhpc                cuentadepruebas   pruebas       gres/gpu        0 \n</code></pre> <pre><code>my_account -a &lt;account_name&gt;[,&lt;account_name&gt;]* -u &lt;user_name&gt;[,&lt;user_name&gt;]* -t\n</code></pre> Example Output <pre><code>[cpetre@srvlogin02 ~]$ my_account -a cuentadepruebas,acct_tests_1 -u cpetre,pruebas -t\n--------------------------------------------------------------------------------\nCluster/Account/User Utilization 1970-01-01T00:00:00 - 2025-10-27T23:59:59 (1761609600 secs)\nUsage reported in TRES Hours\n--------------------------------------------------------------------------------\nCluster                        Account     Login      TRES Name     Used \n--------- ------------------------------ --------- -------------- -------- \nuc3mhpc                   acct_tests_1    cpetre            cpu        1 \nuc3mhpc                   acct_tests_1    cpetre       gres/gpu        0 \nuc3mhpc                cuentadepruebas    cpetre            cpu     7242 \nuc3mhpc                cuentadepruebas    cpetre       gres/gpu        2 \nuc3mhpc                cuentadepruebas   pruebas            cpu       44 \nuc3mhpc                cuentadepruebas   pruebas       gres/gpu        0 \n</code></pre>"},{"location":"user_docs/apptainer/","title":"Apptainer Official Documentation","text":"<p>For tutorials and help with Apptainer check the official documentation.</p>"},{"location":"user_docs/cluster_access/","title":"Cluster Access","text":"<p>In this page you will find information about how to connect to C3. You can do it either via SSH (CLI) or RDP (GUI), but you must be connected to eduroam or the UC3M VPN first.</p> <p>Once your C3 access has been approved you will receive a welcome email with a temporary password. Make sure you log in through SSH the first time, as you will be prompted to change your password. You cannot log in via RDP with the temporary password.</p>"},{"location":"user_docs/cluster_access/#password-policy","title":"Password Policy","text":"<p>All user passwords must follow this policy:</p> <ul> <li>Must be at least 40 characters long.</li> <li>Must include upper and lower-case characters, digits and special characters.</li> <li>Must be renewed every 90 days.</li> <li>You cannot reuse passwords.</li> </ul>"},{"location":"user_docs/cluster_access/#login-nodes","title":"Login Nodes","text":"<p>C3 is equipped with 2 login nodes.</p> <p>The preferred way to access the cluster is through <code>c3.uc3m.es</code>, but you can also connect directly to <code>login[02-03].c3.uc3m.es</code> if available.</p>"},{"location":"user_docs/cluster_access/#ssh","title":"SSH","text":"<p>This is the main way to connect to C3. Open a terminal window and connect to C3 via SSH with your username like this: <pre><code>ssh &lt;user_name&gt;@c3.uc3m.es\n</code></pre> You can also use something like PuTTY to connect to C3 instead of running the <code>ssh</code> command directly.</p> <p>We recommend you to generate a key pair on your local machine to avoid having to type in your password every time you log in.</p> Linux/macOSWindows (PowerShell) <pre><code># Generate the SSH private/public key pair\nssh-keygen -t ed25519 -C \"$(whoami)@$(hostname)-$(date +'%d-%m-%Y')\" -f ~/.ssh/id_ed25519\n\n# Now copy the public key\ncat ~/.ssh/id_ed25519.pub\n</code></pre> <pre><code># Generate the SSH private/public key pair\nssh-keygen -t ed25519 -C \"$env:USERNAME@$env:COMPUTERNAME-$(Get-Date -Format 'dd-MM-yyyy')\" -f \"$env:USERPROFILE\\.ssh\\id_ed25519\"\n\n# Now copy the public key\ntype \"$env:USERPROFILE\\.ssh\\id_ed25519.pub\"\n</code></pre> Your public key should look like this <pre><code>ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIG1IrqJzJwncJte06FkDJjG8wsjcNEXd1/jXOe6g+90M user@hostname-28-10-2025\n</code></pre> <p>Finally log into the cluster and paste your public key in your ~/.ssh/authorized_keys file: <pre><code>nano ~/.ssh/authorized_keys\n</code></pre> Now you should be able to log into C3 without a password.</p>"},{"location":"user_docs/cluster_access/#rdp","title":"RDP","text":"<p>All login nodes provide a graphical environment. You will need an RDP client to log in in this way. You can use whichever you prefer, but here are some recommendations:</p> <ul> <li>Thincast Client: cross-platform</li> <li>Remmina: Linux only</li> <li>Remote Desktop Connection (mstsc.exe): Windows only, usually comes preinstalled</li> </ul> <p>Install and launch your RDP client. We will use Thincast Client for this example. Just fill out the hostname and username fields, save your settings and connect.</p> <p></p> <p>You can adjust your display settings in the Display tab if you need to.</p> Expired Passwords <p>If you are trying to log in via RDP with an expired password, you might encounter issues. Remember that if you log in via SSH with an expired password, you will automatically be prompted to reset it.</p>"},{"location":"user_docs/cluster_access/#transfer-files","title":"Transfer files","text":"<p>If you need to transfer files from your computer to C3, there are many ways to do it. Here we will go over a few.</p> SFTP (CLI)SCP (CLI)WinSCP (Windows GUI) <p>Open a terminal (bash/zsh on Linux/macOS, PowerShell on Windows) and run this command with your username: <pre><code>sftp &lt;user_name&gt;@c3.uc3m.es\n</code></pre></p> Now you have an SFTP shell from which you can upload and download files to and from the cluster Linux/macOSWindows (PowerShell) <pre><code>local_username@local_hostname:~$ sftp c3_username@c3.uc3m.es\nConnected to c3.uc3m.es.\nsftp&gt; \n</code></pre> <pre><code>PS C:\\Users\\local_username&gt; sftp c3_username@c3.uc3m.es\nConnected to c3.uc3m.es.\nsftp&gt;\n</code></pre> <p>To upload a single file from your computer to the cluster: <pre><code>put &lt;local_file_path&gt; &lt;remote_file_path&gt;\n</code></pre></p> Example output Linux/macOSWindows (PowerShell) <pre><code>sftp&gt; put /home/local_username/test/test_file1 /home/c3_username/\nUploading /home/local_username/test/test_file1 to /home/c3_username/test_file1\ntest_file1                                                                        100%    6     1.7KB/s   00:00    \n</code></pre> <pre><code>sftp&gt; put C:/Users/local_username/Documents/test/test_file1.txt /home/c3_username\nUploading C:/Users/local_username/Documents/test/test_file1.txt to /home/c3_username/test_file1.txt\ntest_file1.txt                                                                        100%    8     1.6KB/s   00:00\n</code></pre> <p>To upload an entire directory: <pre><code>put -r &lt;local_file_path&gt; &lt;remote_file_path&gt;\n</code></pre></p> Example output Linux/macOSWindows (PowerShell) <pre><code>sftp&gt; put -r /home/local_username/test/test_dir1/ /home/c3_username/\nUploading /home/local_username/test/test_dir1/ to /home/c3_username/test_dir1\nEntering /home/local_username/test/test_dir1/\ntest_file2                                                                        100%    7     1.7KB/s   00:00    \ntest_file3                                                                        100%    7     2.1KB/s   00:00    \n</code></pre> <pre><code>sftp&gt; put -r C:\\Users\\local_username\\Documents\\test\\test_dir1 /home/c3_username\nUploading C:/Users/local_username/Documents/test/test_dir1/ to /home/c3_username/test_dir1\nEntering C:/Users/local_username/Documents/test/test_dir1/\ntest_file2.txt                                                                        100%    8     3.9KB/s   00:00\ntest_file3.txt                                                                        100%    8     2.0KB/s   00:00\n</code></pre> <p>To download a single file from the cluster to your computer: <pre><code>get &lt;remote_file_path&gt; &lt;local_file_path&gt;\n</code></pre></p> Example output Linux/macOSWindows (PowerShell) <pre><code>sftp&gt; get /home/c3_username/test_file4 /home/local_username/test/\nFetching /home/c3_username/test_file4 to /home/local_username/test/test_file4\ntest_file4                                                                        100%   14     2.4KB/s   00:00    \n</code></pre> <pre><code>sftp&gt; get /home/c3_username/test_file4 C:\\Users\\local_username\\Documents\\test\nFetching /home/c3_username/test_file4 to C:/Users/local_username/Documents/test/test_file4\ntest_file4                                                                            100%   16     1.7KB/s   00:00\n</code></pre> <p>To download an entire directory: <pre><code>get -r &lt;remote_file_path&gt; &lt;local_file_path&gt;\n</code></pre></p> Example output Linux/macOSWindows (PowerShell) <pre><code>sftp&gt; get -r /home/c3_username/test_dir2/ /home/local_username/test/\nFetching /home/c3_username/test_dir2/ to /home/local_username/test/test_dir2\nRetrieving /home/c3_username/test_dir2\ntest_file5                                                                        100%   16     3.1KB/s   00:00    \ntest_file6                                                                        100%   16     3.1KB/s   00:00    \ntest_file7                                                                        100%   16     3.1KB/s   00:00    \n</code></pre> <pre><code>sftp&gt; get -r /home/c3_username/test_dir2 C:/Users/local_username/Documents/test\nFetching /home/c3_username/test_dir2/ to C:/Users/local_username/Documents/test/test_dir2\nRetrieving /home/c3_username/test_dir2\ntest_file5                                                                            100%   16     2.0KB/s   00:00\ntest_file6                                                                            100%   16     2.0KB/s   00:00\ntest_file7                                                                            100%   16     2.2KB/s   00:00\n</code></pre> <p>Open a terminal (bash/zsh on Linux/macOS, PowerShell on Windows).</p> <p>To upload a single file from your computer to the cluster: <pre><code>scp &lt;local_file_path&gt; &lt;user_name&gt;@c3.uc3m.es:&lt;remote_file_path&gt;\n</code></pre></p> Example output Linux/macOSWindows (PowerShell) <pre><code>local_username@local_hostname:~$ scp /home/local_username/test/test_file1  c3_username@c3.uc3m.es:/home/c3_username/test\ntest_file1                                                                        100%    8     2.5KB/s   00:00    \n</code></pre> <pre><code>PS C:\\Users\\local_username&gt; scp C:\\Users\\local_username\\Documents\\test\\test_file1.txt c3_username@c3.uc3m.es:/home/c3_username/test\ntest_file1.txt                                                                                          100%    8     1.3KB/s   00:00\n</code></pre> <p>To upload an entire directory: <pre><code>scp -r &lt;local_file_path&gt; &lt;user_name&gt;@c3.uc3m.es:&lt;remote_file_path&gt;\n</code></pre></p> Example output Linux/macOSWindows (PowerShell) <pre><code>local_username@local_hostname:~$ scp -r /home/local_username/test/test_dir1/  c3_username@c3.uc3m.es:/home/c3_username/test\ntest_file2                                                                        100%    8     2.1KB/s   00:00    \ntest_file3                                                                        100%    8     2.7KB/s   00:00    \n</code></pre> <pre><code>PS C:\\Users\\local_username&gt; scp -r C:\\Users\\local_username\\Documents\\test\\test_dir1\\ c3_username@c3.uc3m.es:/home/c3_username/test\ntest_file2.txt                                                                                          100%    8     1.6KB/s   00:00\ntest_file3.txt                                                                                          100%    8     1.6KB/s   00:00\n</code></pre> <p>To download a single file from the cluster to your computer: <pre><code>scp &lt;user_name&gt;@c3.uc3m.es:&lt;remote_file_path&gt; &lt;local_file_path&gt;\n</code></pre></p> Example output Linux/macOSWindows (PowerShell) <pre><code>local_username@local_hostname:~$ scp c3_username@c3.uc3m.es:/home/c3_username/test/test_file4 /home/local_username/test\ntest_file4                                                                        100%   16     3.0KB/s   00:00    \n</code></pre> <pre><code>PS C:\\Users\\local_username&gt; scp c3_username@c3.uc3m.es:/home/c3_username/test/test_file4 C:/Users/local_username/Documents/test/\ntest_file4                                                                                              100%   16     2.0KB/s   00:00\n</code></pre> <p>To download an entire directory: <pre><code>scp -r &lt;user_name&gt;@c3.uc3m.es:&lt;remote_file_path&gt; &lt;local_file_path&gt;\n</code></pre></p> Example output Linux/macOSWindows (PowerShell) <pre><code>local_username@local_hostname:~$ scp -r c3_username@c3.uc3m.es:/home/c3_username/test/test_dir2 /home/local_username/test\ntest_file5                                                                        100%   16     2.9KB/s   00:00    \ntest_file6                                                                        100%   16     2.9KB/s   00:00    \ntest_file7                                                                        100%   16     3.0KB/s   00:00    \n</code></pre> <pre><code>PS C:\\Users\\local_username&gt; scp -r c3_username@c3.uc3m.es:/home/c3_username/test/test_dir2 C:/Users/local_username/Documents/test/\ntest_file5                                                                                              100%   16     1.7KB/s   00:00\ntest_file6                                                                                              100%   16     2.0KB/s   00:00\ntest_file7                                                                                              100%   16     2.0KB/s   00:00\n</code></pre> <p>If you prefer to use a GUI you can try WinSCP. Once you install it on your local Windows machine launch it and you will see a login window like this. Fill out the Host name and User name fields. </p> <p>If you wish to log in with SSH keys instead of your password, click on the Advanced... button. Now go to SSH/Authentication and under Authentication parameters select your Private key file. </p> <p>You will need a PuTTY formatted key file. If you select your normal private key file you will be asked to convert it. This will create a new key file in PuTTY format and select it. </p> <p></p> <p>Next click OK and save your settings by clicking the Save button. Now you can log in and start transferring files.</p>"},{"location":"user_docs/docker2apptainer/","title":"Docker to Apptainer","text":""},{"location":"user_docs/docker2apptainer/#from-docker-to-apptainer","title":"From Docker to Apptainer","text":""},{"location":"user_docs/docker2apptainer/#convert-a-dockerfile-into-a-def-apptainer-file","title":"Convert a Dockerfile into a .def apptainer file","text":"<p>Dockerfiles and Apptainer <code>.def</code> files are recipes for building container images. </p> <p>Dockerfile is used by Docker/Podman. Their images are layered\u2014every instruction (RUN, COPY, etc.) creates a new layer. It needs a daemon (the Docker engine) to build and run. By default, containers run as root inside the container, though this can be restricted. Images are stored in Docker registries (docker.io and private registries).</p> <p>Meanwhile, a <code>.def</code> file (short for definition file) is the recipe used to build a portable and immutable Apptainer (Singularity) image (.sif). No root is needed to run it, which is safe for HPC systems. The <code>.def</code> file has a section-based structure (%post, %environment, %files, etc.). It is composed of: </p> <ul> <li>the base OS or container (e.g., ubuntu, alpine, fedora)--often bootstrapped from Docker images (you can reuse Dockerfiles indirectly)</li> <li>software to install (e.g., python, nano, wget);</li> <li>environment variables; </li> <li>files to copy inside; and </li> <li>metadata about the container.</li> </ul> <p>For example, if we have the next Dockerfile:</p> <pre><code>#CI base \nFROM ubuntu:16.04 \n#Software dependencies installation\nRUN apt-get update &amp;&amp; apt-get install build-essential -y &amp;&amp; apt-get install nano -y \n#Copy the app code to the container \nCOPY . /home/yourFolder/ \nWORKDIR /home/yourFolder/ \nRUN make clean \nRUN make\n</code></pre> <p><code>.def</code> file equivalent to the Dockerfile would be:</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:16.04\n\n%post\n    # Update and install dependencies\n    apt-get update &amp;&amp; \\\n    apt-get install -y build-essential nano &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n    # Go to working directory\n    cd /home/yorFolder\n\n    # Compile the code\n    make clean\n    make\n\n%files\n    . /home/yourFolder\n\n%workdir\n    /home/yourFolder\n\n%runscript\n    # Default action when you run `apptainer run image.sif`\n    exec ./main \"$@\"\n</code></pre> <p>Thus, a <code>.def</code> file is a recipe file for building containers and it specifies the following:</p> <ul> <li><code>Bootstrap / From</code>: specifies the container's base image, similar to using 'FROM' in a Dockerfile in Docker. It defines the starting environment for building the container.</li> <li><code>%labels</code>: specifies the metadata.</li> <li><code>%environment</code>: defines the sets environment variables.</li> <li><code>%post</code>: defines the commands run at build time (like RUN in Dockerfile).</li> <li><code>%files</code>: copy files from host into container.</li> <li><code>%runscript</code>: specifies the default command when you run the container.</li> <li><code>%help</code>: apptainer help image <code>.sif</code>.</li> </ul>"},{"location":"user_docs/docker2apptainer/#differences-between-a-dockerfile-and-a-def-apptainer-file","title":"Differences between a Dockerfile and a .def Apptainer file","text":"<p>Apptainer <code>.def</code>  files describe images that build into a single portable <code>.sif</code> file that can be run without root privileges, which is why HPC systems prefer Apptainer.</p> Feature Dockerfile Apptainer <code>.def</code> Base image <code>FROM ubuntu:22.04</code> <code>Bootstrap: docker</code> + <code>From: ubuntu:22.04</code> Build system Built with <code>docker build</code>, produces layered image Built with <code>apptainer build</code>, produces a single <code>.sif</code> file File copies <code>COPY ./src /app</code> <code>%files</code> section Environment <code>ENV VAR=value</code> <code>%environment</code> section Run commands (build-time) <code>RUN apt-get install \u2026</code> <code>%post</code> section Default runtime command <code>CMD [\"python\",\"app.py\"]</code> <code>%runscript</code> section Execution model Runs as root inside container unless restricted By default runs as the invoking user, so safer (no root in container) Images Layered, stored in Docker daemon Single compressed <code>.sif</code> file, portable (can copy with <code>scp</code>)"},{"location":"user_docs/docker2apptainer/#build-an-apptainer-image-sif","title":"Build an apptainer image (.sif)","text":"<p>To build a docker image we use the next command:</p> <pre><code>docker build -t image:name ./myfolder\n</code></pre> <p>The equivalent command to build an apptainer image would be:</p> <pre><code>apptainer build myImage.sif myfile.def\n</code></pre> <p>In this command you must indicate the name of a <code>.sif</code> image that you want to build and the <code>.def</code> file used to build the <code>.sif</code> image. </p> <p>Note: always run the build from the directory containing your source code and Makefile. Otherwise, %files won\u2019t copy anything, and you\u2019ll see the \u201ccannot stat\u201d error.</p> <p>If you have a docker image, you can use this image and convert it into a <code>.sif</code> file with the next command:</p> <p><pre><code>apptainer build imageName.sif docker-daemon://docker:image\n</code></pre> Where:</p> <ul> <li><code>docker-daemon://&lt;image&gt;:&lt;tag&gt;</code> tells Apptainer to grab the image from your local Docker daemon (not from DockerHub).</li> <li><code>imageName.sif</code> is the Apptainer <code>.sif</code> output file you\u2019ll be able to use with apptainer build, exec, instance or apptainer run.</li> </ul>"},{"location":"user_docs/docker2apptainer/#use-sif-image-to-build-containers-or-instances","title":"Use .sif image to build containers or instances","text":"<p>To run a container using a <code>.sif</code> image, you can use the next command:</p> <pre><code>apptainer run myImage.sif\n</code></pre> <p>If you need to run a command inside of the container, you can use:</p> <pre><code>apptainer exec myImage.sif ./command arg1 arg2\n</code></pre> <p>If you need to open a shell inside of the container, you can use:</p> <pre><code>apptainer shell myImage.sif\n</code></pre> <p>Note: Unlike Docker, Apptainer does not support named background containers, so the --name and -d flags -- commonly used in the Docker commands -- don\u2019t exist. Each apptainer exec runs the container fresh. </p> <p>Also, Apptainer containers are ephemeral by default \u2014 each exec runs a fresh environment, so no --name or long-running container concept like Docker. Thus, Apptainer images are immutable and ephemeral, so you cannot \u201cname\u201d a running instance.  However, you manage multiple instances either by different bind mounts, different PIDs, or passing an ID argument.</p> <p>To start an instance of the Apptainer image, and have that instance execute your program inside itself -- like to how Docker runs containers in detached mode -- follow the next steps:</p> <ol> <li>Start an instance </li> </ol> <p>An instance is like a long-running container that you can exec commands into multiple times:</p> <pre><code>apptainer instance start myImage.sif myInstanceName\n</code></pre> <p>Where: * <code>myImage.sif</code>  is the SIF image. * <code>myInstanceName</code> is the instance name (like a static container name in Docker).</p> <p>Once generated the Apptainer instance, you can check running instances: <pre><code>apptainer instance list\n</code></pre></p> <ul> <li> <p>Also, you can create multiple instances of the same image, giving each a unique instance name: <pre><code>sudo apptainer instance start myImage.sif myInstanceName0\nsudo apptainer instance start myImage.sif myInstanceName1\n</code></pre></p> </li> <li> <p>Execute code inside the instance</p> </li> </ul> <p>Once generated the Apptainer instance, you can run your program inside it. For example:</p> <pre><code>apptainer exec instance://myInstanceName ./command arg1 arg2 &gt; ./myFolder/myfile.txt\n</code></pre> <p>Where: * <code>instance://myInstanceName</code> tells Apptainer to run the command inside the already running instance. * All relative paths (./myFolder/myfile.txt) are relative to the container\u2019s filesystem.</p> <ol> <li>Stop the instance when done</li> </ol> <pre><code>sudo apptainer instance stop myInstanceName\n</code></pre> <p>Note: There is no need to bind-mount files if everything your program needs is already present inside the <code>.sif</code> image. But, if you want to share files from the host (as a volume in Docker), you can use --bind:</p> <pre><code>sudo apptainer exec --bind /host/myfolder:/home/myFolder/sink instance://myInstanceName ./main ...\n</code></pre>"},{"location":"user_docs/docker2apptainer/#differences-between-run-exec-and-instance-commands","title":"Differences between run, exec and instance commands","text":"<ol> <li>apptainer run</li> <li>Runs the container\u2019s default command (defined in %runscript in the <code>.def</code> file).</li> <li>It\u2019s like docker run IMAGE.<ul> <li>Example: <pre><code>apptainer run myImageName.sif\n</code></pre></li> </ul> </li> </ol> <p>If %runscript in the <code>.def</code> has: <pre><code>%runscript\n    echo \"Hello from my container\"\n</code></pre> - You must use it when you want to define the default container behavior.</p> <ol> <li>apptainer exec</li> <li>Runs a specific command inside the container.</li> <li>It\u2019s like docker exec CONTAINER command.<ul> <li>Example: <pre><code>apptainer exec myImageName.sif python3 script.py\n</code></pre></li> </ul> </li> <li> <p>You must use it when you need to run custom commands or bypass the default %runscript.</p> </li> <li> <p>apptainer instance</p> </li> <li> <p>Apptainer supports long-lived containers, called instances (like Docker containers that keep running).</p> <ul> <li> <p>Start an instance: <pre><code>apptainer instance start myImageName.sif myInstance\n</code></pre> This launches a background container named myInstance.</p> </li> <li> <p>Run commands inside that instance: <pre><code>apptainer exec instance://myinstance python3 script.py\n</code></pre></p> </li> <li> <p>Stop it: <pre><code>apptainer instance stop myInstance\n</code></pre></p> </li> <li>You must use instances when you want persistent state (e.g., web server, simulation workers) instead of one-off execution.</li> </ul> </li> </ol>"},{"location":"user_docs/docker2apptainer/#docker-vs-apptainer-commands-comparison","title":"Docker vs Apptainer: commands comparison","text":"Action Docker Apptainer Run default command <code>docker run IMAGE</code> <code>apptainer run image.sif</code> Run custom command <code>docker run IMAGE command</code> OR <code>docker exec CONTAINER command</code> <code>apptainer exec image.sif command</code> Start background container <code>docker run -d --name NAME IMAGE</code> <code>apptainer instance start image.sif NAME</code> Execute inside background container <code>docker exec NAME command</code> <code>apptainer exec instance://NAME command</code> Stop background container <code>docker stop NAME</code> <code>apptainer instance stop NAME</code> <p>Where: - <code>run</code>: one-shot, default script. - <code>exec</code>: run any command inside. - <code>instance</code>: start/stop long-running containers (like daemons or workers).</p> <p>Note: Apptainer does not have a direct equivalent of Docker Compose. Docker Compose is designed for orchestrating multiple long-running containers (such as services), while Apptainer is intended for HPC/scientific computing, where containers typically run single applications or batch jobs.</p>"},{"location":"user_docs/install_custom_sw_spack/","title":"Install your own SW with Spack","text":"<p>You can install your own software with Spack. First install Spack in your HOME directory. We will refer to this Spack instance as local Spack to distinguish it from the cluster-provided Spack or global Spack. <pre><code>git clone --depth=2 --branch=releases/v1.0 https://github.com/spack/spack.git ~/spack\n</code></pre></p> <p>Activate your local Spack instance: <pre><code>SPACK_LOCAL_ROOT=${HOME}/spack\nsource ${SPACK_LOCAL_ROOT}/share/spack/setup-env.sh\n</code></pre> You can put those two lines in your ~/.bashrc instead of the SPACK_GLOBAL_ROOT ones to have your local Spack automatically activated every time you log in.</p> <p>You will see that there is nothing installed initially: <pre><code>spack find\n</code></pre></p>"},{"location":"user_docs/install_custom_sw_spack/#access-global-spack-packages-from-your-local-spack","title":"Access Global Spack packages from your Local Spack","text":"<p>If you want to use packages from the global Spack instance alongside those installed by you, run this command. It will create a YAML file that configures your local Spack to see the global Spack packages as well: <pre><code>cat &lt;&lt; EOF &gt; ${HOME}/spack/etc/spack/defaults/upstreams.yaml\nupstreams:\n    spack-instance-1:\n        install_tree: /opt/ohpc/pub/spack/opt/spack\nEOF\n</code></pre></p> <p>Now you should see all the global packages: <pre><code>spack find\n</code></pre></p>"},{"location":"user_docs/install_custom_sw_spack/#before-installing-select-compilers","title":"Before installing: select compilers","text":"<p>Before you start installing packages you should check which compilers are available to Spack. To list them: <pre><code>spack compilers\n</code></pre></p> <p>We recommend adding the AOCC compiler (clang-based, AMD-optimized). It is available from the global Spack instance. Load it and add it: <pre><code>spack load aocc@5.0.0\nspack compiler find\n</code></pre></p> <p>Now it should show up: <pre><code>spack compilers\n</code></pre></p>"},{"location":"user_docs/install_custom_sw_spack/#installing-packages","title":"Installing packages","text":"<p>Now we can start installing packages.</p>"},{"location":"user_docs/install_custom_sw_spack/#look-up-packages","title":"Look up packages","text":"<p>List all installable packages: <pre><code>spack list\n</code></pre></p> <p>You can also search for a package: <pre><code>spack list &lt;package name&gt;\n</code></pre></p> Example <pre><code>spack list mpi\n</code></pre> <p>Once you find the package you want to install check its information (source, versions, compilation variants): <pre><code>spack info &lt;package&gt;\n</code></pre></p>"},{"location":"user_docs/install_custom_sw_spack/#basic-installation","title":"Basic installation","text":"<p>Install a package: <pre><code>spack install &lt;package&gt;\n</code></pre></p> Example <pre><code>spack install mpich\n</code></pre> <p>If you want a specific version: Install a package: <pre><code>spack install &lt;package&gt;@&lt;version&gt;\n</code></pre></p> Example <pre><code>spack install mpich@4.2.3\n</code></pre> <p>You can also tell Spack which compiler to use. We recommend using AOCC to produce AMD-optimized code: <pre><code>spack install &lt;package&gt;@&lt;version&gt; %&lt;compiler&gt;\n</code></pre></p> Example <pre><code>spack install mpich@4.2.3 %aocc\n</code></pre> <p>Since there are multiple GCC versions available, you might want to specify a compiler version: <pre><code>spack install &lt;package&gt;@&lt;version&gt; %&lt;compiler&gt;@&lt;version&gt;\n</code></pre></p> Example <pre><code>spack install mpich@4.2.3 %gcc@12.2.0\n</code></pre>"},{"location":"user_docs/install_custom_sw_spack/#advanced-installation","title":"Advanced installation","text":"<p>Now we will specify some compilation options using Spack variants. With <code>spack info</code> you can see all the available variants of a package. Some can be turned on and off (true or false), others have named options.</p> <p>Following the mpich example, let us check some of its variants: <pre><code>spack info mpich\n</code></pre> <pre><code>Variants:\n    argobots [false]                false, true\n        Enable Argobots support\n    build_system [autotools]        autotools\n        Build systems supported by the package\n    cuda [false]                    false, true\n        Build with CUDA\n    device [ch4]                    ch3, ch3:sock, ch4\n        Abstract Device Interface (ADI)\n        implementation. The ch4 device is in experimental state for versions\n        before 3.4.\n</code></pre> Each variant has a default value; for example cuda is disabled by default and device is configured as ch4.</p> <p>Install a package with specific variants: <pre><code>spack install &lt;package&gt;@&lt;version&gt; +&lt;variant set to true&gt; ~&lt;variant set to false&gt; &lt;variant&gt;=&lt;value&gt; %&lt;compiler&gt;@&lt;version&gt;\n</code></pre></p> Example <p>Let us say we want to install mpich with argobots on, cuda off and device as \"ch3\": <pre><code>spack install mpich@4.2.3 +argobots ~cuda device=ch3 %aocc@5.0.0\n</code></pre></p> <p>You can also specify version and variants for dependencies: <pre><code>spack install &lt;package&gt;@&lt;version&gt; &lt;variant&gt;=&lt;value&gt; ^&lt;dependency&gt;@&lt;dependency version&gt;&lt;dependency variants&gt; %&lt;compiler&gt;@&lt;version&gt;\n</code></pre></p> Example <p>Let us say we want to install mpich with argobots on, cuda off and device as \"ch3\": <pre><code>spack install mpich@4.2.3 netmod=ucx ^ucx@1.18.0 %aocc@5.0.0\nspack install mpich@4.2.3 netmod=ucx ^ucx@1.18.0+cma~dm %aocc@5.0.0\n</code></pre></p> <p>For more details please check the official Spack documentation.</p>"},{"location":"user_docs/installed_sw/","title":"Installed Software","text":"<p>Software packages on C3 are available as Lmod modules and/or Spack packages. Here we will cover how to use the provided software environment.</p>"},{"location":"user_docs/installed_sw/#modules-lmod","title":"Modules (Lmod)","text":"<p>To list available modules: <pre><code>module avail\n</code></pre></p> Default module version <p>If there are multiple versions of the same module, one of them will be the default version, marked with a (D). If you load a module without specifying a version the default will be chosen.</p> <p>To check which modules are currently loaded: <pre><code>module list\n</code></pre></p> <p>To load a module: <pre><code>module load &lt;module_name&gt;\n</code></pre></p> <p>To load a specific version: <pre><code>module load &lt;module_name&gt;/&lt;version&gt;\n</code></pre></p> Example <pre><code>module load cuda/11.0\n</code></pre> <p>To unload a module: <pre><code>module unload &lt;module_name&gt;\n</code></pre></p> <p>To swap one module for another: <pre><code>module swap &lt;module_name&gt;/&lt;version&gt; &lt;module_name&gt;/&lt;version&gt;\n</code></pre></p> Example <pre><code>module switch cuda/12.8 cuda/11.0\n</code></pre> <p>To unload all modules: <pre><code>module purge\n</code></pre></p> <p>To get information about a module: <pre><code>module spider &lt;module_name&gt;\n</code></pre></p> Example <pre><code>[user@srvlogin03 ~]$ module spider cuda\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------\ncuda:\n--------------------------------------------------------------------------------------------------------------------------------------------------------\n    Versions:\n        cuda/11.0\n        cuda/12.8\n    Other possible modules matches:\n        nvhpc-hpcx-2.20-cuda12  nvhpc-hpcx-cuda12\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------\nTo find other possible module matches execute:\n\n    $ module -r spider '.*cuda.*'\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------\nFor detailed information about a specific \"cuda\" package (including how to load the modules) use the module's full name.\nNote that names that have a trailing (E) are extensions provided by other modules.\nFor example:\n\n    $ module spider cuda/12.8\n--------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>For more information please check the official Lmod documentation.</p>"},{"location":"user_docs/installed_sw/#spack","title":"Spack","text":"<p>Many software packages on the cluster are available through Spack. To access them you must activate the cluster Spack instance, aka global Spack: <pre><code>SPACK_GLOBAL_ROOT=/opt/ohpc/pub/spack\nsource ${SPACK_GLOBAL_ROOT}/share/spack/setup-env.sh\n</code></pre> You can put those two lines in your ~/.bashrc to have global Spack automatically activated every time you log in.</p> <p>Now we will go through some commonly used Spack commands. You can check the official Spack documentation for more details.</p>"},{"location":"user_docs/installed_sw/#basic-commands","title":"Basic commands","text":"<p>List available packages: <pre><code>spack find\n</code></pre></p> <p>Load a package: <pre><code>spack load &lt;package&gt;\n</code></pre></p> <p>Check which packages you have loaded: <pre><code>spack find --loaded\n</code></pre></p> <p>Unload a package: <pre><code>spack unload &lt;package&gt;\n</code></pre></p>"},{"location":"user_docs/installed_sw/#advanced-commands","title":"Advanced commands","text":"<p>List available packages with more details (variants, i.e. compilation options): <pre><code>spack find -v\n</code></pre></p> <p>Check package information (source, versions, variants): <pre><code>spack info &lt;package&gt;\n</code></pre></p> <p>List packages that were compiled with a specific compiler: <pre><code>spack find %&lt;compiler&gt;@&lt;version&gt;\n</code></pre></p> Example <pre><code>spack find %gcc@12.2.0\n</code></pre> <p>List available packages with their installation path: <pre><code>spack find -p\n</code></pre></p> <p>There could be multiple instances of the same package, with different versions and/or variants. Each package has a unique hash to identify it. List packages with hash: <pre><code>spack find -l\n</code></pre></p> <p>Load a specific package instance using the hash: <pre><code>spack load &lt;package&gt;/&lt;hash&gt;\n</code></pre></p> <p>Let us say you need a specific version of mpich that uses UCX. First list the available packages: <pre><code>spack find -vl mpich\n</code></pre></p> Available MPICH packages <pre><code>-- linux-rocky8-zen3 / aocc@5.0.0 -------------------------------\nulhdpg3 mpich@4.2.2~argobots~cuda+fortran~hcoll+hwloc+hydra~level_zero+libxml2+pci~rocm+romio~slurm+vci+verbs+wrapperrpath~xpmem build_system=autotools datatype-engine=auto device=ch4 netmod=ucx pmi=default\nr4faawj mpich@4.2.2~argobots~cuda+fortran+hwloc+hydra~level_zero+libxml2+pci~rocm+romio~slurm+vci+verbs+wrapperrpath~xpmem build_system=autotools datatype-engine=auto device=ch4 netmod=ofi pmi=default\n==&gt; 2 installed packages\n</code></pre> <p>Now load the UCX one: <pre><code>spack load mpich/ulhdpg3\n</code></pre></p>"},{"location":"user_docs/installed_sw/#custom-software","title":"Custom Software","text":"<p>If you need additional software, you can try to install it through Spack following this guide.</p> <p>If you cannot install it on your own, please contact us at c3-uc3m@uc3m.es</p>"},{"location":"user_docs/jupyter/","title":"JupyterLab","text":"<p>This is a simple guide to launch a JupyterLab instance from a GPU node.</p> <p>To execute JupyterLab instances on a GPU node you have to first create a Conda environment following these instructions. Note: you only have to do this once. <pre><code># Create a conda environment\nmodule load anaconda\nconda create --name lab python=3.12\nconda activate lab\n\n# Install dependencies\nconda install -y conda-forge::tensorflow-gpu conda-forge::cuda conda-forge::cudnn\npip install jupyterlab ipykernel\n</code></pre></p> <p>To launch your JupyterLab instance first allocate a GPU job: <pre><code># Launch a Slurm job on a GPU node\nsrun -A &lt;slurm_account&gt; -p gpu -N 1 --gpus=1 --pty bash\n</code></pre></p> <p>Now launch your JupyterLab instance: <pre><code># Launch Jupyter\nmodule load anaconda\nconda activate lab\njupyter lab --no-browser --ip=$(host $(hostname) | awk '/10.119.12/ {print $4}')\n</code></pre></p>"},{"location":"user_docs/matlab/","title":"Parallel Computing with MATLAB on the C3 HPC Cluster","text":"<p>This document provides the steps to configure MATLAB to submit jobs to the C3 cluster, retrieve results, and debug errors.</p> Compatible MATLAB versions <p>R2025a</p> <p>R2025b</p>"},{"location":"user_docs/matlab/#initial-configuration","title":"Initial Configuration","text":"Running MATLAB on the HPC Cluster <p>This setup is intended for job submission when you are logged directly into the cluster, either through a command-line or graphical interface.  This process needs to be done once per cluster.</p> <p>After logging into the cluster, start MATLAB.  On the Home tab, click <code>Parallel &gt; Discover Clusters\u2026</code> to discover the profile.</p> <p><p> </p></p> <p><p> </p></p> <p>Follow the prompts to create a new cluster profile.  Jobs will run across multiple nodes on the cluster rather than on the host machine.</p> Running MATLAB on the Desktop <p>This setup is intended for job submission when MATLAB is installed on your machine and jobs are run remotely on the cluster. You can download MATLAB here. This setup needs to be done once per cluster, per version of MATLAB installed on your machine.</p> <p>Start MATLAB and run <code>userpath</code></p> <pre><code>userpath\n</code></pre> <p>Download the Cluster MATLAB support package from here. The contents of the ZIP file should be extracted into the folder returned by the call to <code>userpath</code>.</p> <p>Create a new cluster profile <pre><code>configCluster\n</code></pre></p> <p>Submission to the cluster requires SSH credentials. You will be prompted for username and password or identity file (private key). The username and location of the private key will be stored in MATLAB for future sessions.</p> <p>Jobs will now run on the cluster rather than on the local machine.</p> <p>NOTE: To run jobs on the local machine instead of the cluster, use the Process profile.</p> <pre><code>% Get a handle to the local resources\nc = parcluster('Processes');\n</code></pre>"},{"location":"user_docs/matlab/#alternate","title":"Alternate","text":"<p>After logging into the cluster, start MATLAB. Call <code>configCluster</code> to create a new cluster profile.</p> <pre><code>configCluster\n</code></pre> <p>Jobs will run across multiple nodes on the cluster rather than on the host machine.</p>"},{"location":"user_docs/matlab/#configuring-jobs","title":"Configuring Jobs","text":"<p>Prior to submitting the job, various scheduler flags can be assigned, such as queue, e-mail, walltime, etc.  [Only AccountName is required.]</p> <pre><code>% Get a handle to the cluster\nc = parcluster;\n\n% REQUIRED\n\n% Specify an account\nc.AdditionalProperties.AccountName = 'account-name';\n\n% OPTIONAL\n\n% Specify a constraint\nc.AdditionalProperties.Constraint = 'feature-name';\n\n% Request email notification of job status\nc.AdditionalProperties.EmailAddress = 'user-id@uc3m.es';\n\n% Specify number of GPUs (default: 0)\nc.AdditionalProperties.GPUsPerNode = 1;\n\n% Specify a particular GPU card\nc.AdditionalProperties.GPUCard = 'gpu-card';\n\n% Specify memory to use, per core (default: 4GB)\nc.AdditionalProperties.MemPerCPU = '6GB';\n\n% Specify the partition\nc.AdditionalProperties.Partition = 'partition-name';\n\n% Specify cores per node\nc.AdditionalProperties.ProcsPerNode = 4;\n\n% Set node exclusivity (default: false)\nc.AdditionalProperties.RequireExclusiveNode = true;\n\n% Specify a reservation\nc.AdditionalProperties.Reservation = 'reservation-name';\n\n% Specify the wall time (e.g., 1 day, 5 hours, 30 minutes)\nc.AdditionalProperties.WallTime = '1-05:30';\n</code></pre> <p>To persist changes made to <code>AdditionalProperties</code> between MATLAB sessions, save the profile</p> <pre><code>c.saveProfile\n</code></pre> <p>To see the values of the current configuration options, display <code>AdditionalProperties</code>.</p> <pre><code>c.AdditionalProperties\n</code></pre> <p>Unset a value when no longer needed.</p> <pre><code>% Turn off email notifications\nc.AdditionalProperties.EmailAddress = '';\n\n% Don't request an entire node\nc.AdditionalProperties.RequireExclusiveNode = false;\n</code></pre>"},{"location":"user_docs/matlab/#independent-batch-job-matlab-on-the-hpc-cluster-or-desktop","title":"Independent Batch Job - MATLAB on the HPC Cluster or Desktop","text":"<p>Use the <code>batch</code> command to submit asynchronous jobs to the cluster. The <code>batch</code> command will return a job object which is used to access the output of the submitted job. See the MATLAB documentation for more help on <code>batch</code>.</p> <pre><code>% Get a handle to the cluster\nc = parcluster;\n\n% Submit job to query where MATLAB is running on the cluster\njob = c.batch(@pwd, 1, {}, 'CurrentFolder', '.');\n\n% Query job for state\njob.State\n\n% If job is finished, fetch the results\njob.fetchOutputs{1}\n\n% Delete the job after results are no longer needed\njob.delete\n</code></pre> <p>To retrieve a list of running or completed jobs, call <code>parcluster</code> to return the cluster object. The cluster object stores an array of jobs that are listed as <code>queued</code>, <code>running</code>, <code>finished</code>, or <code>failed</code>. Retrieve and view the list of jobs as shown below.</p> <pre><code>c = parcluster;\njobs = c.Jobs\n\n% Get a handle to the second job in the list\njob2 = c.Jobs(2);\n</code></pre> <p>Once the job has been selected, fetch the results as previously done.</p> <p><code>fetchOutputs</code> is used to retrieve function output arguments; if calling <code>batch</code> with a script, use <code>load</code> instead. Data that has been written to disk on the cluster needs to be retrieved directly from the file system (e.g., via sftp).</p> <pre><code>% Fetch all results from the second job in the list\njob2.fetchOutputs{:}\n\n% Alternate: Load results if job was a script instead of a function\njob2.load\n</code></pre>"},{"location":"user_docs/matlab/#parallel-batch-job-matlab-on-the-hpc-cluster-or-desktop","title":"Parallel Batch Job - MATLAB on the HPC Cluster or Desktop","text":"<p><code>The</code> <code>batch</code> command can also support parallel workflows. Let\u2019s use the following example for a parallel job, which you should save separately as <code>parallel_example.m</code>.</p> <pre><code>function [sim_t, A] = parallel_example(iter)\n\nif nargin==0\n    iter = 8;\nend\n\ndisp('Start sim')\n\nt0 = tic;\nparfor idx = 1:iter\n    A(idx) = idx;\n    pause(2)\n    idx\nend\nsim_t = toc(t0);\n\ndisp('Sim completed')\n\nsave RESULTS A\n\nend\n</code></pre> <p>This time when using the <code>batch</code> command, also specify a Pool argument.</p> <pre><code>% Get a handle to the cluster\nc = parcluster;\n\n% Submit a batch pool job using 4 workers for 16 simulations\njob = c.batch(@parallel_example, 1, {16}, 'CurrentFolder','.', 'Pool', 4);\n\n% View current job status\njob.State\n\n% Fetch the results after a finished state is retrieved\njob.fetchOutputs{1}\nans =\n    8.8872\n</code></pre> <p>The job ran in 8.89 seconds using four workers. Note that these jobs will always request <code>N+1</code> CPU cores, since one worker is required to manage the batch job and pool of workers. For example, a job that needs eight workers will request nine CPU cores.</p> <p>Run the same simulation again but increase the Pool size. This time, to retrieve the results later, keep track of the job ID.</p> <p>NOTE: For some applications, there will be a diminishing return when allocating too many workers, as the overhead may exceed computation time.</p> <pre><code>% Get a handle to the cluster\nc = parcluster;\n\n% Submit a batch pool job using 8 workers for 16 simulations\njob = c.batch(@parallel_example, 1, {16}, 'CurrentFolder','.', 'Pool', 8);\n\n% Get the job ID\nid = job.ID\nid =\n    4\n\n% Clear job from workspace (as though MATLAB exited)\nclear job\n</code></pre> <p>With a handle to the cluster, the <code>findJob</code> method searches for the job with the specified job ID.</p> <pre><code>% Get a handle to the cluster\nc = parcluster;\n\n% Find the old job\njob = c.findJob('ID', 4);\n\n% Retrieve the state of the job\njob.State\nans =\n    finished\n\n% Fetch the results\njob.fetchOutputs{1}\nans =\n    4.7270\n</code></pre> <p>The job now runs in 4.73 seconds using eight workers. Run code with different number of workers to determine the ideal number to use.</p> <p>Alternatively, to retrieve job results via a graphical user interface, use the Job Monitor (<code>Parallel &gt; Monitor Jobs</code>).</p> <p> </p> <p> </p> Debugging <p>If a serial job produces an error, call the <code>getDebugLog</code> method to view the error log file.</p> <p>When submitting an independent job, specify the task. <pre><code>c.getDebugLog(job.Tasks)\n</code></pre></p> <p>For Pool jobs, only specify the job object. <pre><code>c.getDebugLog(job)\n</code></pre></p> <p>When troubleshooting a job, the cluster admin may request the scheduler ID of the job. This can be derived by calling <code>getTaskSchedulerIDs</code>. <pre><code>job.getTaskSchedulerIDs()\nans =\n    25539\n</code></pre></p> Helper Functions Function  Description  Notes  clusterFeatures  Lists cluster features/constraints  clusterGpuCards  Lists cluster GPU cards  clusterPartitionNames  Lists cluster partition/queue names  disableArchiving  Modifies file archiving to resolve file mirroring issues  Applicable only to Desktop  fixConnection  Reestablishes cluster connection (e.g., after reconnection of VPN)  Applicable only to Desktop  willRun  Explains why job is queued"},{"location":"user_docs/matlab/#to-learn-more","title":"To Learn More","text":"<p>To learn more about the MATLAB Parallel Computing Toolbox, check out these resources:</p> <ul> <li>Parallel Computing Overview </li> <li>Parallel Computing Documentation </li> <li>Parallel Computing Coding Examples </li> <li>Parallel Computing Tutorials </li> <li>Parallel Computing Videos </li> <li>Parallel Computing Webinars </li> <li>Demo Script</li> <li>MATLAB and Simulink Access for UC3M</li> </ul>"},{"location":"user_docs/slurm/","title":"Slurm","text":"<p>SLURM (Simple Linux Utility for Resource Management) is a cluster management and job scheduling system used in high-performance computing (HPC) clusters. This manual covers the basic commands for submitting and managing jobs in our SLURM environment.</p>"},{"location":"user_docs/slurm/#basic-commands","title":"Basic Commands","text":"<ul> <li>sinfo: Displays cluster nodes status</li> <li>sbatch: Run a job in batch mode</li> <li>srun: Executes an interactive job or a job step within a script/sbatch job</li> <li>squeue: Shows active and queued jobs</li> <li>scancel: Cancel a job</li> <li>sacct: Shows job history</li> </ul>"},{"location":"user_docs/slurm/#accounting","title":"Accounting","text":"<p>In order to launch jobs with Slurm (sbatch, salloc, srun) you must specify a billing account aka Slurm account with the -A or --account options. You can check which Slurm account your project is associated to like this: <pre><code>sacctmgr show user $USER withassoc format=User,Account%30\n</code></pre></p> Example output <pre><code>[pruebas@srvlogin02 ~]$ sacctmgr show user $USER withassoc format=User,Account%30\n    User                        Account  \n---------- ------------------------------  \n  pruebas                cuentadepruebas\n</code></pre>"},{"location":"user_docs/slurm/#common-pitfalls","title":"Common pitfalls","text":"<p>If the name of the Slurm account is too long it could be displayed with a + sign at the end, like this: <pre><code>[pruebas@srvlogin02 ~]$ sacctmgr show user $USER withassoc format=User,Account\n     User    Account  \n---------- ----------  \n  pruebas cuentadep+\n</code></pre> In this example, cuentadep+ is not a valid Slurm account! If you try to launch jobs with cuentadep+ you will get an error. The correct account is cuentadepruebas (the full name).</p> <p>To ensure that you get the full account name, specify a longer length for the account, e.g. 40 characters (Account%40) until you don\u2019t see a + sign at the end: <pre><code>sacctmgr show user $USER withassoc format=User,Account%40\nsacctmgr show user $USER withassoc format=User,Account%&lt;number of characters&gt;\n</code></pre></p> Multiple Slurm Accounts <p>If you are part of more than one project you will have one user login with access to multiple Slurm accounts. Let us take Bob for an example: he is a researcher working on 2 different projects, Project-Apples and Project-Oranges. Bob has a single user login (bob) with access to 2 Slurm accounts: project_apples and project_oranges. When Bob works on Project-Apples he should use the corresponding project_apples Slurm account, like this: <pre><code>srun -A project_apples --pty bash\n</code></pre></p> <p>When you launch jobs please make sure to assign them to the correct Slurm account.</p>"},{"location":"user_docs/slurm/#ease-of-use","title":"Ease of use","text":"<p>If you only have one Slurm account we recommend that you add these lines at the end of your ~/.bashrc. Every time you log in this will export an environment variable that contains your account name, so that you can reference it easily. Just replace cuentadepruebas with your full Slurm account name. <pre><code># set up Slurm Account\nexport SLURM_BILLING_ACCOUNT=cuentadepruebas\n</code></pre></p> <p>Now you can launch jobs like this: <pre><code>srun -A $SLURM_BILLING_ACCOUNT --pty bash\n</code></pre></p> Billing for Exclusive Jobs <p>If you submit jobs with the <code>--exclusive</code> flag, you will be billed for all node resources (all CPU cores and all GPUs), even if you specify a limited number of cores/GPUs.</p>"},{"location":"user_docs/slurm/#launch-jobs","title":"Launch Jobs","text":"<p>There are several ways to launch jobs in Slurm. Here we cover the two main approaches.</p>"},{"location":"user_docs/slurm/#interactive-jobs","title":"Interactive Jobs","text":"<p>Interactive mode is useful for tests, development or if you just need to work directly on a compute node.</p> <p>Example 1. Simple interactive session: Requests default resources and opens a bash shell in the allocated compute node <pre><code>srun -A &lt;slurm account&gt; --pty bash\n</code></pre></p> <p>Example 2. Specify resources: This requests 1 node, 4 processes (cores), 1 hour runtime and 4GB of RAM <pre><code>srun --account &lt;slurm account&gt; --pty --nodes=1 --ntasks=4 --time=01:00:00 --mem=4G bash\n</code></pre> For more advanced customization please check the official Slurm documentation for srun.</p>"},{"location":"user_docs/slurm/#batch-jobs","title":"Batch Jobs","text":"<p>Ideal for production workloads or long jobs. We have a script that contains all job instructions.</p> <p>Example sbatch script (my_job.sbatch). Replace &lt;slurm account&gt; with your Slurm account <pre><code>#!/bin/bash\n\n#SBATCH --job-name=my_job\n#SBATCH --account=&lt;slurm account&gt;\n#SBATCH --output=output_%j.out\n#SBATCH --error=error_%j.err\n#SBATCH --ntasks=4\n#SBATCH --time=02:00:00\n#SBATCH --mem=8G\n\n# load modules (optional)\nmodule load python/3.10\n\n# run the program\npython my_script.py\n</code></pre></p> <p>For more advanced customization please check the official Slurm documentation for sbatch.</p>"},{"location":"user_docs/slurm/#monitor-jobs","title":"Monitor Jobs","text":"<p>Display all running/queued jobs <pre><code>squeue\n</code></pre></p> <p>Display only your jobs <pre><code>squeue -u $USER\n</code></pre></p> <p>Check job details <pre><code>scontrol show job &lt;job_id&gt;\n</code></pre></p> <p>Cancel job <pre><code>scancel &lt;job_id&gt;\n</code></pre></p> <p>List job history <pre><code>sacct -u $USER\n</code></pre></p>"},{"location":"user_docs/slurm/#additional-documentation","title":"Additional Documentation","text":"<p>If you need help with something that is not covered in this guide please check the official Slurm Documentation.</p>"},{"location":"blog/archive/2025/","title":"November 2025","text":""},{"location":"blog/category/visit/","title":"Visit","text":""},{"location":"blog/category/welcome/","title":"Welcome","text":""}]}