{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"General Information","text":"<p>The UC3M Scientific Computing Center is a computing and storage service center for R&amp;D&amp;i groups, public research organizations, and companies involved in research and innovation.</p> <p>The C3 provides computing power, storage, and technical support to its users to promote and participate in the development of research and technological development projects, as well as to contribute to the development and strengthening of its users' competitive capacity.</p> <p>The C3's computers include computing and storage capacity that exceeds the resources available in standard systems, making them particularly suitable for numerical simulations, biocomputing, and artificial intelligence, given their volume of resources, both in CPUs and in reliable storage capacity.</p> <p>Before using the C3 services you have to register as a UC3M CAI user. Please follow the instructions here.</p> <p></p>"},{"location":"about/","title":"About","text":"<p>SCientific Computing Center (C3) of University Carlos III of Madrid is part of the Research Support Center (CAI). The CAI's mission is to provide researchers with a technological support center that develops and offers the methodologies and resources necessary for the execution of their research and development projects.</p> <p>The C3 has been created by University Carlos III of Madrid and has received funding from Programa Estatal para Impulsar la Investigaci\u00f3n Cient\u00edfico-T\u00e9cnica of the Agencia Estatal de Investigaci\u00f3n (AEI) through the project \"Centro Para El An\u00e1lisis Y Modelado De Sistemas Complejos En Ingenier\u00eda Y Biomedicina\" with reference EQC2021-007184-P, and through the PREDCOV project, which was funded by an agreement between Comunidad de Madrid (Consejer\u00eda de Educaci\u00f3n, Universidades, Ciencia y Portavoc\u00eda) and the University Carlos III of Madrid to develop research projects concerning SARS-COV 2 and the COVID-19 disease financed with the REACT-UE resources from the European Regional Development Fund (ERDF).</p>"},{"location":"courses/","title":"HPC Courses","text":"<p>A collection of recommended HPC courses.</p>"},{"location":"courses/#an-introduction-to-high-performance-computing","title":"An introduction to High Performance Computing","text":""},{"location":"courses/#introduction-to-high-performance-computing","title":"Introduction to High Performance Computing","text":""},{"location":"courses/#hpc-parallel-programming-resources","title":"HPC &amp; Parallel programming resources","text":""},{"location":"courses/#introduction-to-parallel-programming-with-mpi-and-openmp","title":"Introduction to parallel programming with MPI and OpenMP","text":""},{"location":"courses/#introduction-to-python","title":"Introduction to Python","text":""},{"location":"courses/#high-performance-computing-with-python","title":"High-Performance Computing with Python","text":""},{"location":"courses/#using-ipython-for-parallel-computing","title":"Using IPython for Parallel Computing","text":""},{"location":"courses/#high-performance-scientific-computing-in-c","title":"High-performance scientific computing in C++","text":""},{"location":"courses/#prace-training-course-directive-based-gpu-programming-with-openacc","title":"PRACE Training Course: Directive-based GPU programming with OpenACC","text":""},{"location":"courses/#practical-high-performance-computing-system-administration","title":"Practical: High-Performance Computing System Administration","text":""},{"location":"resources/installed_sw/","title":"Installed Software","text":"<p>Here you will find a list of available software.</p> Tool Version Source Tau 2.31.1 Lmod (tau/2.31.1) LINPACK 2.3.0 Spack (hpl@2.3) IOR 3.3.0 Spack (ior@3.3.0) OpenMP gcc-12.2.0 Lmod (gnu12/12.2.0) OpenMP gcc-14.2.0 Spack (gcc@14.2.0) OpenMP aocc-5.0.0 Spack (aocc@5.0.0) OpenMP aocc-5.0.0 Lmod (aocc-5.0.0) OpenMPI 5.0.7 Lmod (openmpi/5.0.7-ofi) OpenMPI 5.0.7 Lmod (openmpi/5.0.7-ucx) OpenMPI 4.1.4 Lmod (openmpi/4.1.4) OpenMPI 5.0.6 Spack (openmpi@5.0.6) MPICH 4.3.0 Lmod (mpich/4.3.0-ofi) MPICH 4.3.0 Lmod (mpich/4.3.0-ucx) MPICH 3.4.3 Lmod (mpich/3.4.3-ofi) MPICH 3.4.3 Lmod (mpich/3.4.3-ucx) MPICH 4.2.2 Spack (mpich@4.2.2/r4faawj) MPICH 4.2.2 Spack (mpich@4.2.2/ulhdpg3) UCX 1.18.0 Lmod (ucx/1.18.0) UCX 1.11.2 Lmod (ucx/1.11.2) UCX 1.18.0 Spack (ucx@1.18.0) GNU compiler C/C++/ Fortran 12.2.0 Lmod (gnu12/12.2.0) GNU compiler C/C++/ Fortran 14.2.0 Spack (gcc@14.2.0) NVIDIA HPC-SDK 25.1 Lmod NVIDIA HPC-X 2.22 Lmod (hpcx) Python 3.12.0 Lmod (python/3.12.0) Python 3.11.3 Lmod (python/3.11.3) Python 3.13.2 Spack (python@3.13.2) Python 3.11.9 Spack (python@3.11.9) Python 3.10.16 Spack (python@3.10.16) Anaconda 3 Lmod (anaconda/3) lib GSL 2.7.1 Lmod (gsl/2.7.1) lib xml2 2.13.5 Spack (libxml2@2.13.5) HDF5 1.10.8 Lmod (hdf5/1.10.8) HDF5 1.14.5 Spack (hdf5@1.14.5) ArrayFire 3.8.1 Spack (arrayfire@3.8.1) Apptainer 1.4.1 Local vim nan Local emacs nan Local (login nodes only) FFTW 3.3.10 Lmod (fftw/3.3.10) AMD-FFTW 5.0 Spack (amdfftw@5.0) NetCDF 4.9.0 Lmod (netcdf/4.9.0) PETSc 3.18.1 Lmod (petsc/3.18.1) superLU 5.2.1 Lmod (superlu/5.2.1) openBLAS 0.3.21 Lmod (openblas/0.3.21) openBLAS 0.3.29 Spack (openblas@0.3.29) AMDBLIS 5.0. Spack (amdblis@5.0) LAPACK 3.12.1 Spack (netlib-lapack@3.12.1) AOCL-libFLAME 5.0. Spack (amdlibflame@5.0) FlexiBLAS 3.4.2 Spack (flexiblas@3.4.2) Apache Arrow 18.0.0 Spack (arrow@18.0.0) git nan Local git 2.47.0 Spack (git@2.47.0) autoconf 2.72 Spack(autoconf@2.72) automake 1.16.5 Spack(automake@1.16.5) cmake 3.24.2 Lmod (cmake/3.24.2) cmake 3.31.5 Spack (cmake@3.31.5) libtool 2.4.7 Spack (libtool@2.4.7) flex 2.6.4 Spack (flex@2.6.4) bison 3.8.2 Spack (bison@3.8.2) R 4.2.1 Lmod (R/4.2.1) julia 1.11.2 Spack (julia@1.11.2) lib geos 3.13.0 Spack (geos@3.13.0) lib gdal 3.10.1 Spack (gdal@3.10.1) lib proj 9.4.1 Spack (proj@9.4.1) lib sqlite3 3.46.0 Spack (sqlite@3.46.0) lib udunits2 2.2.28 Spack (udunits@2.2.28) lib curl 8.11.1 Spack (curl@8.11.1) lib openssl 3.4.0 Spack (openssl@3.4.0) lib boost 1.80.0 Lmod (boost/1.80.0) lib boost 1.87.0 Spack (boost@1.87.0) lib glpk 5.0 Spack (glpk@5.0) lib gmp 6.3.0 Spack (gmp@6.3.0) lib mpfr 4.2.1 Spack (mpfr@4.2.1) lib tbb 2022.0.0 Spack (intel-tbb@2022.0.0) ODBC 1.3-23 Spack (r-rodbc@1.3-23) ODBC 2.3.12 Spack (unixodbc@2.3.12) ffmpeg 7.1 Spack (ffmpeg@7.1) MATLAB R2025a Lmod (matlab/R2025a) MATLAB R2025b Lmod (matlab/R2025b)"},{"location":"resources/slurm/","title":"Slurm Partitions","text":""},{"location":"resources/slurm/#general-partions-cpu-only","title":"General partions (CPU-only)","text":"<ul> <li>cpu:<ul> <li>Nodes: 90 (srv[01-45,101-145])</li> <li>Time limit: 2 hours</li> </ul> </li> <li>batch:<ul> <li>Nodes: 90 (srv[01-45,101-145])</li> <li>Time limit: 2 days</li> </ul> </li> <li>large:<ul> <li>Nodes: 90 (srv[01-45,101-145])</li> <li>Time limit: 14 days</li> </ul> </li> </ul>"},{"location":"resources/slurm/#gpu-partitions","title":"GPU partitions","text":"<ul> <li>gpu:<ul> <li>Nodes: 5 (srvgpu[01-05])</li> <li>Time limit: 2 hours</li> <li>GPUs: 37</li> </ul> </li> <li>gpu-batch:<ul> <li>Nodes: 5 (srvgpu[01-05])</li> <li>Time limit: 2 days</li> <li>GPUs: 37</li> </ul> </li> <li>gpu-large:<ul> <li>Nodes: 5 (srvgpu[01-05])</li> <li>Time limit: 14 days</li> <li>GPUs: 37</li> </ul> </li> </ul>"},{"location":"resources/system/","title":"System Description","text":"<p>The C3 supercomputing cluster has the following technical specifications:</p> <ul> <li> <p>90 compute nodes with the following specifications per node:</p> <ul> <li>Dual processor backplane with two AMD EPYC 7713 CPUs, each with 64 cores (128 cores total).</li> <li>1024GB DDR4-3200 ECC RDIMM RAM.</li> <li>3.5TB NVMe PCIe Gen 4.0 local SSD storage for scratch space.</li> </ul> </li> <li> <p>5 GPU nodes with the following specifications per node:</p> <ul> <li>Dual processor motherboard with two AMD EPYC 7513 CPUs, each with 32 cores (64 cores total).</li> <li>512GB DDR4 3200 ECC RDIMM RAM.</li> <li>7TB NVMe PCIe Gen 4.0 local SSD storage for scratch space.</li> </ul> </li> <li> <p>42 NVIDIA A40 GPUs (336 Tensor Cores, 84 RT Cores and 48 GB GDDR6 ECC memory each)</p> </li> <li> <p>High availability storage system featuring:</p> <ul> <li>Lustre FS, with 540TB of capacity and high redundancy.</li> <li>NFS with DRAM access, and 64TB of capacity.</li> </ul> </li> <li> <p>100Gbps HDR Infiniband network for computation and data storage.</p> </li> </ul>"},{"location":"user_docs/apptainer/","title":"Apptainer Official Documentation","text":"<p>For tutorials and help with Apptainer check the official documentation.</p>"},{"location":"user_docs/cluster_access/","title":"Docs","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/cluster_access/#cluster-access","title":"Cluster Access","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/cluster_access/#ssh","title":"SSH","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/cluster_access/#rdp","title":"RDP","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/cluster_access/#transfer-files","title":"Transfer files","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/docker2apptainer/","title":"Docker to Apptainer","text":""},{"location":"user_docs/docker2apptainer/#from-docker-to-apptainer","title":"From Docker to Apptainer","text":""},{"location":"user_docs/docker2apptainer/#convert-a-dockerfile-into-a-def-apptainer-file","title":"Convert a Dockerfile into a .def apptainer file","text":"<p>Dockerfiles and Apptainer <code>.def</code> files are recipes for building container images. </p> <p>Dockerfile is used by Docker/Podman. Their images are layered\u2014every instruction (RUN, COPY, etc.) creates a new layer. It needs a daemon (the Docker engine) to build and run. By default, containers run as root inside the container, though this can be restricted. Images are stored in Docker registries (docker.io and private registries).</p> <p>Meanwhile, a <code>.def</code> file (short for definition file) is the recipe used to build a portable and immutable Apptainer (Singularity) image (.sif). No root is needed to run it, which is safe for HPC systems. The <code>.def</code> file has a section-based structure (%post, %environment, %files, etc.). It is composed of: </p> <ul> <li>the base OS or container (e.g., ubuntu, alpine, fedora)--often bootstrapped from Docker images (you can reuse Dockerfiles indirectly)</li> <li>software to install (e.g., python, nano, wget);</li> <li>environment variables; </li> <li>files to copy inside; and </li> <li>metadata about the container.</li> </ul> <p>For example, if we have the next Dockerfile:</p> <pre><code>#CI base \nFROM ubuntu:16.04 \n#Doftware dependecies installation\nRUN apt-get update &amp;&amp; apt-get install build-essential -y &amp;&amp; apt-get install nano -y \n#Copy the app code to the container \nCOPY . /home/yourFolder/ \nWORKDIR /home/yourFolder/ \nRUN make clean \nRUN make\n</code></pre> <p><code>.def</code> file equivalent to the Dockerfile would be:</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:16.04\n\n%post\n    # Update and install dependencies\n    apt-get update &amp;&amp; \\\n    apt-get install -y build-essential nano &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n    # Go to working directory\n    cd /home/yorFolder\n\n    # Compile the code\n    make clean\n    make\n\n%files\n    . /home/yourFolder\n\n%workdir\n    /home/yourFolder\n\n%runscript\n    # Default action when you run `apptainer run image.sif`\n    exec ./main \"$@\"\n</code></pre> <p>Thus, a <code>.def</code> file is a recipe file for building containers and it specifies the following:</p> <ul> <li><code>Bootstrap / From</code>: specifies the container's base image, similar to using 'FROM' in a Dockerfile in Docker. It defines the starting environment for building the container.</li> <li><code>%labels</code>: specifie the metadata.</li> <li><code>%environment</code>: defines the sets environment variables.</li> <li><code>%post</code>: defines the commands run at build time (like RUN in Dockerfile).</li> <li><code>%files</code>: copy files from host into container.</li> <li><code>%runscript</code>: specifies the default command when you run the container.</li> <li><code>%help</code>: apptainer help image <code>.sif</code>.</li> </ul>"},{"location":"user_docs/docker2apptainer/#differences-between-a-dockerfile-and-a-def-apptainer-file","title":"Differences between a Dockerfile and a .def Apptainer file","text":"<p>Apptainer <code>.def</code>  files describe images that build into a single portable <code>.sif</code> file that can be run without root privileges, which is why HPC systems prefer Apptainer.</p> Feature Dockerfile Apptainer <code>.def</code> Base image <code>FROM ubuntu:22.04</code> <code>Bootstrap: docker</code> + <code>From: ubuntu:22.04</code> Build system Built with <code>docker build</code>, produces layered image Built with <code>apptainer build</code>, produces a single <code>.sif</code> file File copies <code>COPY ./src /app</code> <code>%files</code> section Environment <code>ENV VAR=value</code> <code>%environment</code> section Run commands (build-time) <code>RUN apt-get install \u2026</code> <code>%post</code> section Default runtime command <code>CMD [\"python\",\"app.py\"]</code> <code>%runscript</code> section Execution model Runs as root inside container unless restricted By default runs as the invoking user, so safer (no root in container) Images Layered, stored in Docker daemon Single compressed <code>.sif</code> file, portable (can copy with <code>scp</code>)"},{"location":"user_docs/docker2apptainer/#build-an-apptainer-image-sif","title":"Build an apptainer image (.sif)","text":"<p>To build a docker image we use the next command:</p> <pre><code>docker build -t image:name ./myfolder\n</code></pre> <p>The equivalent command to build an apptainer image would be:</p> <pre><code>apptainer build myImage.sif myfile.def\n</code></pre> <p>In this command you must indicate the name of a <code>.sif</code> image that you want to build and the <code>.def</code> file used to build the <code>.sif</code> image. </p> <p>Note: always run the build from the directory containing your source code and Makefile. Otherwise, %files won\u2019t copy anything, and you\u2019ll see the \u201ccannot stat\u201d error.</p> <p>If you have a docker image, you can use this image and convert it into a <code>.sif</code> file with the next command:</p> <p><pre><code>apptainer build imageName.sif docker-daemon://docker:image\n</code></pre> Where:</p> <ul> <li><code>docker-daemon://&lt;image&gt;:&lt;tag&gt;</code> tells Apptainer to grab the image from your local Docker daemon (not from DockerHub).</li> <li><code>imageName.sif</code> is the Apptainer <code>.sif</code> output file you\u2019ll be able to use with apptainer build, exec, instance or apptainer run.</li> </ul>"},{"location":"user_docs/docker2apptainer/#use-sif-image-to-build-containers-or-instances","title":"Use .sif image to build containers or instances","text":"<p>To run a container using a <code>.sif</code> image, you can use the next command:</p> <pre><code>apptainer run myImage.sif\n</code></pre> <p>If you need to run a command inside of the container, you can use:</p> <pre><code>apptainer exec myImage.sif ./command arg1 arg2\n</code></pre> <p>If you need to open a shell inside of the container, you can use:</p> <pre><code>apptainer shell myImage.sif\n</code></pre> <p>Note: Unlike Docker, Apptainer does not support named background containers, so the --name and -d flags -- commonly used in the Docker commands -- don\u2019t exist. Each apptainer exec runs the container fresh. </p> <p>Also, Apptainer containers are ephemeral by default \u2014 each exec runs a fresh environment, so no --name or long-running container concept like Docker. Thus, Apptainer images are immutable and ephemeral, so you cannot \u201cname\u201d a running instance.  However, you manage multiple instances either by different bind mounts, different PIDs, or passing an ID argument.</p> <p>To start an instance of the Apptainer image, and have that instance execute your program inside itself -- like to how Docker runs containers in detached mode -- follow the next steps:</p> <ol> <li>Start an instance </li> </ol> <p>An instance is like a long-running container that you can exec commands into multiple times:</p> <pre><code>apptainer instance start myImage.sif myInstanceName\n</code></pre> <p>Where: * <code>myImage.sif</code>  is the SIF image. * <code>myInstanceName</code> is the instance name (like a static container name in Docker).</p> <p>Once generated the Apptainer instance, you can check running instances: <pre><code>apptainer instance list\n</code></pre></p> <ul> <li> <p>Also, you can create multiple instances of the same image, giving each a unique instance name: <pre><code>sudo apptainer instance start myImage.sif myInstanceName0\nsudo apptainer instance start myImage.sif myInstanceName1\n</code></pre></p> </li> <li> <p>Execute code inside the instance</p> </li> </ul> <p>Once generated the Apptainer instance, you can run your program inside it. For example:</p> <pre><code>apptainer exec instance://myInstanceName ./command arg1 arg2 &gt; ./myFolder/myfile.txt\n</code></pre> <p>Where: * <code>instance://myInstanceName</code> tells Apptainer to run the command inside the already running instance. * All relative paths (./myFolder/myfile.txt) are relative to the container\u2019s filesystem.</p> <ol> <li>Stop the instance when done</li> </ol> <pre><code>sudo apptainer instance stop myInstanceName\n</code></pre> <p>Note: There is no need to bind-mount files if everything your program needs is already present inside the <code>.sif</code> image. But, if you want to share files from the host (as a volume in Docker), you can use --bind:</p> <pre><code>sudo apptainer exec --bind /host/myfolder:/home/myFolder/sink instance://myInstanceName ./main ...\n</code></pre>"},{"location":"user_docs/docker2apptainer/#diferences-between-run-exec-and-instance-commands","title":"Diferences between run, exec and instance commands","text":"<ol> <li>apptainer run</li> <li>Runs the container\u2019s default command (defined in %runscript in the <code>.def</code> file).</li> <li>It\u2019s like docker run IMAGE.<ul> <li>Example: <pre><code>apptainer run myImageName.sif\n</code></pre></li> </ul> </li> </ol> <p>If %runscript in the <code>.def</code> has: <pre><code>%runscript\n    echo \"Hello from my container\"\n</code></pre> - You must use it when you want to define the default container behaviour.</p> <ol> <li>apptainer exec</li> <li>Runs a specific command inside the container.</li> <li>It\u2019s like docker exec CONTAINER command.<ul> <li>Example: <pre><code>apptainer exec myImageName.sif python3 script.py\n</code></pre></li> </ul> </li> <li> <p>You must use it when you need to run custom commands or bypass the default %runscript.</p> </li> <li> <p>apptainer instance</p> </li> <li> <p>Apptainer supports long-lived containers, called instances (like Docker containers that keep running).</p> <ul> <li> <p>Start an instance: <pre><code>apptainer instance start myImageName.sif myInstance\n</code></pre> This launches a background container named myInstance.</p> </li> <li> <p>Run commands inside that instance: <pre><code>apptainer exec instance://myinstance python3 script.py\n</code></pre></p> </li> <li> <p>Stop it: <pre><code>apptainer instance stop myInstance\n</code></pre></p> </li> <li>You must use instances when you want persistent state (e.g., web server, simulation workers) instead of one-off execution.</li> </ul> </li> </ol>"},{"location":"user_docs/docker2apptainer/#docker-vs-apptainer-commands-comparation","title":"Docker vs Apptainer: commands comparation","text":"Action Docker Apptainer Run default command <code>docker run IMAGE</code> <code>apptainer run image.sif</code> Run custom command <code>docker run IMAGE command</code> OR <code>docker exec CONTAINER command</code> <code>apptainer exec image.sif command</code> Start background container <code>docker run -d --name NAME IMAGE</code> <code>apptainer instance start image.sif NAME</code> Execute inside background container <code>docker exec NAME command</code> <code>apptainer exec instance://NAME command</code> Stop background container <code>docker stop NAME</code> <code>apptainer instance stop NAME</code> <p>Where: - <code>run</code>: one-shot, default script. - <code>exec</code>: run any command inside. - <code>instance</code>: start/stop long-running containers (like daemons or workers).</p> <p>Note: Apptainer does not have a direct equivalent of Docker Compose. Docker Compose is designed for orchestrating multiple long-running containers (such as services), while Apptainer is intended for HPC/scientific computing, where containers typically run single applications or batch jobs.</p>"},{"location":"user_docs/install_custom_sw_spack/","title":"Install your own SW with Spack","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/installed_sw/","title":"Installed Software","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/installed_sw/#modules","title":"Modules","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/installed_sw/#spack","title":"Spack","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/installed_sw/#custom-software","title":"Custom Software","text":"<p>If you need additional software, you can try to install it through Spack by following this guide.</p> <p>If you cannot install it on your own, please contact us at c3-uc3m@uc3m.es</p>"},{"location":"user_docs/jupyter/","title":"JupyterHub","text":"<p>PLACEHOLDER</p>"},{"location":"user_docs/matlab/","title":"Parallel Computing with MATLAB on the UC3M-HPC Cluster","text":"<p>This document provides the steps to configure MATLAB to submit jobs to a cluster, retrieve results, and debug errors.</p>"},{"location":"user_docs/matlab/#initial-configuration","title":"Initial Configuration","text":"Running MATLAB on the HPC Cluster <p>This setup is intended for job submission when you are logged directly into the cluster, either through a command-line or graphical interface.  This process needs to be done once per cluster.</p> <p>After logging into the cluster, start MATLAB.  On the Home tab, click <code>Parallel &gt; Discover Clusters\u2026</code> to discover the profile.</p> <p><p> </p></p> <p><p> </p></p> <p>Follow the prompts to create a new cluster profile.  Jobs will run across multiple nodes on the cluster rather than on the host machine.</p> Running MATLAB on the Desktop <p>This setup is intended for job submission when MATLAB is installed on your machine and jobs are run remotely on the cluster. You can download MATLAB here. This setup needs to be done once per cluster, per version of MATLAB installed on your machine.</p> <p>Start MATLAB and run <code>userpath</code></p> <pre><code>userpath\n</code></pre> <p>Download the Cluster MATLAB support package from here.  The contents of the ZIP file should be extracted into the folder returned by the call to <code>userpath</code>.</p> <p>Create a new cluster profile <pre><code>configCluster\n</code></pre></p> <p>Submission to the cluster requires SSH credentials. You will be prompted for username and password or identity file (private key). The username and location of the private key will be stored in MATLAB for future sessions.</p> <p>Jobs will now run on the cluster rather than on the local machine.</p> <p>NOTE: To run jobs on the local machine instead of the cluster, use the Process profile.</p> <pre><code>% Get a handle to the local resources\nc = parcluster('Processes');\n</code></pre>"},{"location":"user_docs/matlab/#alternate","title":"Alternate","text":"<p>After logging into the cluster, start MATLAB. Call <code>configCluster</code> to create a new cluster profile.</p> <pre><code>configCluster\n</code></pre> <p>Jobs will run across multiple nodes on the cluster rather than on the host machine.</p>"},{"location":"user_docs/matlab/#configuring-jobs","title":"Configuring Jobs","text":"<p>Prior to submitting the job, various scheduler flags can be assigned, such as queue, e-mail, walltime, etc.  [Only YYY is required OR None of these are required.]</p> <pre><code>% REQUIRED\n\n&lt;a id=\"M_M_M_M_54b8\"&gt;&lt;/a&gt;\n% Specify an account\nc.AdditionalProperties.AccountName = 'account-name';\n\n% OPTIONAL\n\n% Specify a constraint\nc.AdditionalProperties.Constraint = 'feature-name';\n\n% Request email notification of job status\nc.AdditionalProperties.EmailAddress = 'user-id@uc3m.es';\n\n% Specify number of GPUs (default: 0)\nc.AdditionalProperties.GPUsPerNode = 1;\n\n% Specify a particular GPU card\nc.AdditionalProperties.GPUCard = 'gpu-card';\n\n% Specify memory to use, per core (default: 4GB)\nc.AdditionalProperties.MemPerCPU = '6GB';\n\n% Specify the partition\nc.AdditionalProperties.Partition = 'partition-name';\n\n% Specify cores per node (default: 0)\nc.AdditionalProperties.ProcsPerNode = 4;\n\n% Set node exclusivity (default: false)\nc.AdditionalProperties.RequireExclusiveNode = true;\n\n% Specify a reservation\nc.AdditionalProperties.Reservation = 'reservation-name';\n\n% Specify the wall time (e.g., 1 day, 5 hours, 30 minutes)\nc.AdditionalProperties.WallTime = '1-05:30';\n</code></pre> <p>To persist changes made to <code>AdditionalProperties</code> between MATLAB sessions, save the profile</p> <pre><code>c.saveProfile\n</code></pre> <p>To see the values of the current configuration options, display <code>AdditionalProperties</code>.</p> <pre><code>c.AdditionalProperties\n</code></pre> <p>Unset a value when no longer needed.</p> <pre><code>% Turn off email notifications\nc.AdditionalProperties.EmailAddress = '';\n\n% Don't request an entire node\nc.AdditionalProperties.RequireExclusiveNode = false;\n</code></pre>"},{"location":"user_docs/matlab/#interactive-jobs-running-matlab-on-the-hpc-cluster","title":"Interactive Jobs - Running MATLAB on the HPC Cluster","text":"<p>To run an interactive pool job on the cluster, continue to use <code>parpool</code> as before.</p> <pre><code>% Get a handle to the cluster\nc = parcluster;\n\n% Open a pool of 64 workers on the cluster\npool = c.parpool(64);\n</code></pre> <p>Rather than running a local pool on the host machine, the pool can now run across multiple nodes on the cluster.</p> <pre><code>% Run a parfor over 1000 iterations\nparfor idx = 1:1000\n    a(idx) = rand;\nend\n</code></pre> <p>Delete the pool when it\u2019s no longer needed.</p> <pre><code>% Delete the pool\npool.delete\n</code></pre>"},{"location":"user_docs/matlab/#independent-batch-job-matlab-on-the-hpc-cluster-or-desktop","title":"Independent Batch Job - MATLAB on the HPC Cluster or Desktop","text":"<p>Use the <code>batch</code> command to submit asynchronous jobs to the cluster. The <code>batch</code> command will return a job object which is used to access the output of the submitted job. See the MATLAB documentation for more help on <code>batch</code>.</p> <pre><code>% Get a handle to the cluster\nc = parcluster;\n\n% Submit job to query where MATLAB is running on the cluster\njob = c.batch(@pwd, 1, {}, 'CurrentFolder', '.');\n\n% Query job for state\njob.State\n\n% If job is finished, fetch the results\njob.fetchOutputs{1}\n\n% Delete the job after results are no longer needed\njob.delete\n</code></pre> <p>To retrieve a list of running or completed jobs, call <code>parcluster</code> to return the cluster object. The cluster object stores an array of jobs that are listed as <code>queued</code>, <code>running</code>, <code>finished</code>, or <code>failed</code>. Retrieve and view the list of jobs as shown below.</p> <pre><code>c = parcluster;\njobs = c.Jobs\n\n% Get a handle to the second job in the list\njob2 = c.Jobs(2);\n</code></pre> <p>Once the job has been selected, fetch the results as previously done.</p> <p><code>fetchOutputs</code> is used to retrieve function output arguments; if calling <code>batch</code> with a script, use <code>load</code> instead. Data that has been written to disk on the cluster needs to be retrieved directly from the file system (e.g., via sftp).</p> <pre><code>% Fetch all results from the second job in the list\njob2.fetchOutputs{:}\n\n% Alternate: Load results if job was a script instead of a function\njob2.load\n</code></pre>"},{"location":"user_docs/matlab/#parallel-batch-job-matlab-on-the-hpc-cluster-or-desktop","title":"Parallel Batch Job - MATLAB on the HPC Cluster or Desktop","text":"<p><code>The</code> <code>batch</code> command can also support parallel workflows. Let\u2019s use the following example for a parallel job, which you should save separately as <code>parallel_example.m</code>.</p> <pre><code>function [sim_t, A] = parallel_example(iter)\n\nif nargin==0\n    iter = 8;\nend\n\ndisp('Start sim')\n\nt0 = tic;\nparfor idx = 1:iter\n    A(idx) = idx;\n    pause(2)\n    idx\nend\nsim_t = toc(t0);\n\ndisp('Sim completed')\n\nsave RESULTS A\n\nend\n</code></pre> <p>This time when using the <code>batch</code> command, also specify a Pool argument.</p> <pre><code>% Get a handle to the cluster\nc = parcluster;\n\n% Submit a batch pool job using 4 workers for 16 simulations\njob = c.batch(@parallel_example, 1, {16}, 'CurrentFolder','.', 'Pool', 4);\n\n% View current job status\njob.State\n\n% Fetch the results after a finished state is retrieved\njob.fetchOutputs{1}\nans =\n    8.8872\n</code></pre> <p>The job ran in 8.89 seconds using four workers. Note that these jobs will always request <code>N+1</code> CPU cores, since one worker is required to manage the batch job and pool of workers. For example, a job that needs eight workers will request nine CPU cores.</p> <p>Run the same simulation again but increase the Pool size. This time, to retrieve the results later, keep track of the job ID.</p> <p>NOTE: For some applications, there will be a diminishing return when allocating too many workers, as the overhead may exceed computation time.</p> <pre><code>% Get a handle to the cluster\nc = parcluster;\n\n% Submit a batch pool job using 8 workers for 16 simulations\njob = c.batch(@parallel_example, 1, {16}, 'CurrentFolder','.', 'Pool', 8);\n\n% Get the job ID\nid = job.ID\nid =\n    4\n\n% Clear job from workspace (as though MATLAB exited)\nclear job\n</code></pre> <p>With a handle to the cluster, the <code>findJob</code> method searches for the job with the specified job ID.</p> <pre><code>% Get a handle to the cluster\nc = parcluster;\n\n% Find the old job\njob = c.findJob('ID', 4);\n\n% Retrieve the state of the job\njob.State\nans =\n    finished\n\n% Fetch the results\njob.fetchOutputs{1};\nans =\n    4.7270\n</code></pre> <p>The job now runs in 4.73 seconds using eight workers. Run code with different number of workers to determine the ideal number to use.</p> <p>Alternatively, to retrieve job results via a graphical user interface, use the Job Monitor (<code>Parallel &gt; Monitor Jobs</code>).</p> <p> </p> <p> </p> Debugging <p>If a serial job produces an error, call the <code>getDebugLog</code> method to view the error log file.</p> <p>When submitting an independent job, specify the task. <pre><code>c.getDebugLog(job.Tasks)\n</code></pre></p> <p>For Pool jobs, only specify the job object. <pre><code>c.getDebugLog(job)\n</code></pre></p> <p>When troubleshooting a job, the cluster admin may request the scheduler ID of the job. This can be derived by calling <code>getTaskSchedulerIDs</code> (call <code>schedID(job)</code> before R2019b). <pre><code>job.getTaskSchedulerIDs()\nans =\n    25539\n</code></pre></p> Helper Functions Function  Description  Notes  clusterFeatures  Lists cluster features/constraints  clusterGpuCards  Lists cluster GPU cards  clusterPartitionNames  Lists cluster partition/queue names  disableArchiving  Modifies file archiving to resolve file mirroring issues  Applicable only to Desktop  fixConnection  Reestablishes cluster connection (e.g., after reconnection of VPN)  Applicable only to Desktop  willRun  Explains why job is queued"},{"location":"user_docs/matlab/#to-learn-more","title":"To Learn More","text":"<p>To learn more about the MATLAB Parallel Computing Toolbox, check out these resources:</p> <ul> <li>Parallel Computing Overview </li> <li>Parallel Computing Documentation </li> <li>Parallel Computing Coding Examples </li> <li>Parallel Computing Tutorials </li> <li>Parallel Computing Videos </li> <li>Parallel Computing Webinars </li> </ul>"},{"location":"user_docs/slurm/","title":"Slurm","text":"<p>SLURM (Simple Linux Utility for Resource Management) is a cluster management and job scheduling system used in high-performance computing (HPC) clusters. This manual covers the basic commands for submitting and managing jobs in our SLURM environment.</p>"},{"location":"user_docs/slurm/#basic-commands","title":"Basic Commands","text":"<ul> <li>sinfo: Displays cluster nodes status</li> <li>sbatch: Run a job in batch mode</li> <li>srun: Executes an interactive job or a job step within a script/sbatch job</li> <li>squeue: Shows active and queued jobs</li> <li>scancel: Cancel a job</li> <li>sacct: Shows job history</li> </ul>"},{"location":"user_docs/slurm/#accounting","title":"Accounting","text":"<p>In order to launch jobs with Slurm (sbatch, salloc, srun) you must specify a billing account aka Slurm account with the -A or --account options. You can check which Slurm account your project is associated to like this: <pre><code>sacctmgr show user $USER withassoc format=User,Account%30\n</code></pre></p> Example output <pre><code>[pruebas@srvlogin02 ~]$ sacctmgr show user $USER withassoc format=User,Account%30\n    User                        Account  \n---------- ------------------------------  \n  pruebas                cuentadepruebas\n</code></pre>"},{"location":"user_docs/slurm/#common-pitfalls","title":"Common pitfalls","text":"<p>If the name of the Slurm account is too long it could be displayed with a + sign at the end, like this: <pre><code>[pruebas@srvlogin02 ~]$ sacctmgr show user $USER withassoc format=User,Account\n     User    Account  \n---------- ----------  \n  pruebas cuentadep+\n</code></pre> In this example, cuentadep+ is not a valid Slurm account! If you try to launch jobs with cuentadep+ you will get an error. The correct account is cuentadepruebas (the full name).</p> <p>To ensure that you get the full account name, specify a longer length for the account, e.g. 40 characters (Account%40) until you don\u2019t see a + sign at the end: <pre><code>sacctmgr show user $USER withassoc format=User,Account%40\nsacctmgr show user $USER withassoc format=User,Account%&lt;number of characters&gt;\n</code></pre></p> Multiple Slurm Accounts <p>If you are part of more than one project you will have one user login with access to multiple Slurm accounts. Let us take Bob for an example: he is a researcher working on 2 different projects, Project-Apples and Project-Oranges. Bob has a single user login (bob) with access to 2 Slurm accounts: project_apples and project_oranges. When Bob works on Project-Apples he should use the corresponding project_apples Slurm account, like this: <pre><code>srun -A project_apples --pty bash\n</code></pre></p> <p>When you launch jobs please make sure to assign them to the correct Slurm account.</p>"},{"location":"user_docs/slurm/#ease-of-use","title":"Ease of use","text":"<p>If you only have one Slurm account we recommend that you add these lines at the end of your ~/.bashrc. Every time you log in this will export an environment variable that contains your account name, so that you can reference it easily. Just replace cuentadepruebas with your full Slurm account name. <pre><code># set up Slurm Account\nexport SLURM_BILLING_ACCOUNT=cuentadepruebas\n</code></pre></p> <p>Now you can launch jobs like this: <pre><code>srun -A $SLURM_BILLING_ACCOUNT --pty bash\n</code></pre></p>"},{"location":"user_docs/slurm/#launch-jobs","title":"Launch Jobs","text":"<p>There are several ways to launch jobs in Slurm. Here we cover the two main approaches.</p>"},{"location":"user_docs/slurm/#interactive-jobs","title":"Interactive Jobs","text":"<p>Interactive mode is useful for tests, development or if you just need to work directly on a compute node.</p> <p>Example 1. Simple interactive session: Requests default resources and opens a bash shell in the allocated compute node <pre><code>srun -A &lt;slurm account&gt; --pty bash\n</code></pre></p> <p>Example 2. Specify resources: This requests 1 node, 4 processes (cores), 1 hour runtime and 4GB of RAM <pre><code>srun --account &lt;slurm account&gt; --pty --nodes=1 --ntasks=4 --time=01:00:00 --mem=4G bash\n</code></pre> For more advanced customization please check the official Slurm documentation for srun.</p>"},{"location":"user_docs/slurm/#batch-jobs","title":"Batch Jobs","text":"<p>Ideal for production workloads or long jobs. We have a script that contains all job instructions.</p> <p>Example sbatch script (my_job.sbatch). Replace &lt;slurm account&gt; with your Slurm account <pre><code>#!/bin/bash\n\n#SBATCH --job-name=my_job\n#SBATCH --account=&lt;slurm account&gt;\n#SBATCH --output=output_%j.out\n#SBATCH --error=error_%j.err\n#SBATCH --ntasks=4\n#SBATCH --time=02:00:00\n#SBATCH --mem=8G\n\n# load modules (optional)\nmodule load python/3.10\n\n# run the program\npython my_script.py\n</code></pre></p> <p>For more advanced customization please check the official Slurm documentation for sbatch.</p>"},{"location":"user_docs/slurm/#monitor-jobs","title":"Monitor Jobs","text":"<p>Display all running/queued jobs <pre><code>squeue\n</code></pre></p> <p>Display only your jobs <pre><code>squeue -u $USER\n</code></pre></p> <p>Check job details <pre><code>scontrol show job &lt;job_id&gt;\n</code></pre></p> <p>Cancel job <pre><code>scancel &lt;job_id&gt;\n</code></pre></p> <p>List job history <pre><code>sacct -u $USER\n</code></pre></p>"},{"location":"user_docs/slurm/#additional-documentation","title":"Additional Documentation","text":"<p>If you need help with something that is not covered in this guide please check the official Slurm Documentation.</p>"}]}